{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+--------+\n",
      "| Atack type   |   Total |   Train |   Test |\n",
      "+==============+=========+=========+========+\n",
      "| Normal       |   10800 |    8640 |   2160 |\n",
      "+--------------+---------+---------+--------+\n",
      "| Attack       |   89200 |   71360 |  17840 |\n",
      "+--------------+---------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Load dataset and split it into training and test set\n",
    "################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "dataset_name = \"cic-iot\"\n",
    "sample_size = 100000\n",
    "\n",
    "# Load dateset\n",
    "df = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2.csv')\n",
    "\n",
    "# Split dataset according to attack type\n",
    "normal_df = df[df['label'] == 'BenignTraffic']\n",
    "attack_df = df[df['label'] != 'BenignTraffic']\n",
    "\n",
    "# Drop columns\n",
    "normal_df = normal_df.drop(columns=['label'])\n",
    "attack_df = attack_df.drop(columns=['label'])\n",
    "\n",
    "# Split dataset into training and test set\n",
    "normal_df_train = normal_df.sample(frac=0.8, random_state=42)\n",
    "normal_df_test = normal_df.drop(normal_df_train.index)\n",
    "attack_df_train = attack_df.sample(frac=0.8, random_state=42)\n",
    "attack_df_test = attack_df.drop(attack_df_train.index)\n",
    "\n",
    "# Print dataset sizes in a table\n",
    "data = [\n",
    "    [\"Normal\", normal_df.shape[0], normal_df_train.shape[0], normal_df_test.shape[0]],\n",
    "    [\"Attack\", attack_df.shape[0], attack_df_train.shape[0], attack_df_test.shape[0]]\n",
    "]\n",
    "print(tabulate(data, headers=[\"Atack type\", \"Total\", \"Train\", \"Test\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S4025371\\OneDrive - RMIT University\\Repositories\\iot-llm\\.conda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "['flow_duration', 'Header_Length', 'Rate', 'Srate', 'psh_flag_number', 'ack_flag_number', 'syn_count', 'fin_count', 'rst_count', 'Tot sum']\n",
      "```\n",
      "```python\n",
      "['flow_duration', 'Header_Length', 'Rate', 'Srate', 'psh_flag_number', 'ack_flag_number', 'syn_count', 'fin_count', 'rst_count', 'Tot sum']\n",
      "```\n",
      "```python\n",
      "['flow_duration', 'Header_Length', 'Rate', 'Srate', 'psh_flag_number', 'ack_flag_number', 'syn_count', 'fin_count', 'rst_count', 'Tot sum']\n",
      "```\n",
      "```python\n",
      "['flow_duration', 'Header_Length', 'Rate', 'Srate', 'psh_flag_number', 'ack_flag_number', 'syn_count', 'fin_count', 'rst_count', 'Tot sum']\n",
      "```\n",
      "```python\n",
      "['flow_duration', 'Header_Length', 'Rate', 'Srate', 'psh_flag_number', 'ack_flag_number', 'syn_count', 'fin_count', 'rst_count', 'Tot sum']\n",
      "```\n",
      "```python\n",
      "['flow_duration', 'Header_Length', 'Rate', 'Srate', 'psh_flag_number', 'ack_flag_number', 'syn_count', 'fin_count', 'rst_count', 'Tot sum']\n",
      "```\n",
      "```python\n",
      "['flow_duration', 'Header_Length', 'Rate', 'Srate', 'psh_flag_number', 'ack_flag_number', 'syn_count', 'fin_count', 'rst_count', 'Tot sum']\n",
      "```\n",
      "```python\n",
      "['flow_duration', 'Header_Length', 'Rate', 'Srate', 'psh_flag_number', 'ack_flag_number', 'syn_count', 'fin_count', 'rst_count', 'Tot sum']\n",
      "```\n",
      "```python\n",
      "['flow_duration', 'Header_Length', 'Rate', 'Srate', 'psh_flag_number', 'ack_flag_number', 'syn_count', 'fin_count', 'rst_count', 'Tot sum']\n",
      "```\n",
      "```python\n",
      "['flow_duration', 'Header_Length', 'Rate', 'Srate', 'psh_flag_number', 'ack_flag_number', 'syn_count', 'fin_count', 'rst_count', 'Tot sum']\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Generate Feature Importance\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Output top 10 important features that can be used to filter an entry as either normal or attack.\n",
    "Output only in the Python list structure.\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\n",
    "Example output:\n",
    "['feature1', 'feature2', 'feature3', ..., 'feature10']\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "model_name = \"gpt-4o\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "# llm = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "# model_name = \"claude-3-opus-20240229\"\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=dataset_name,\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "\n",
    "normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "normal_entries = {}\n",
    "for i, feature_name in enumerate(normal_df_train.columns.to_list()):\n",
    "    normal_entries[feature_name] = [json.loads(doc)[i] for doc in normal_documents]\n",
    "\n",
    "attack_entries = {}\n",
    "for i, feature_name in enumerate(attack_df_train.columns.to_list()):\n",
    "    attack_entries[feature_name] = [json.loads(doc)[i] for doc in attack_documents]\n",
    "\n",
    "completions = []\n",
    "for i in range(10):\n",
    "    completion = chain.invoke({\n",
    "        \"normal_entries\": json.dumps(normal_entries),\n",
    "        \"attack_entries\": json.dumps(attack_entries)\n",
    "    })\n",
    "    completions.append(completion.content)\n",
    "    print(completion.content)\n",
    "    time.sleep(10)\n",
    "\n",
    "with open(f\"results/feature-importance-{sample_size}-llm-{model_name}.txt\", \"a\") as f:\n",
    "    f.write(\"\\n\".join(completions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S4025371\\OneDrive - RMIT University\\Repositories\\iot-llm\\.conda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"flow_duration\": \"if flow_duration < 1.0 then attack else normal\",\n",
      "    \"Header_Length\": \"if Header_Length < 1000 then attack else normal\",\n",
      "    \"Rate\": \"if Rate < 10.0 then attack else normal\",\n",
      "    \"ack_flag_number\": \"if ack_flag_number == 0 then attack else normal\",\n",
      "    \"HTTPS\": \"if HTTPS == 0 then attack else normal\"\n",
      "}\n",
      "```\n",
      "Prompt tokens: 5353\n",
      "Completion tokens: 94\n",
      "Total tokens: 5447\n",
      "Percentage of tokens used: 0.0425546875\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Generate Rules with transposed data\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import uuid\n",
    "import tiktoken     # https://github.com/openai/tiktoken\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Generate 5 simple and deterministic rules for top 5 important features to filter an entry as either normal or attack. \n",
    "Output only in the JSON format with the structure: \n",
    "{{'feature1': 'rule', 'feature2': 'rule', ..., 'feature5': 'rule'}}.\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "model_name = \"gpt-4o\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "# llm = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "# model_name = \"claude-3-opus-20240229\"\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=dataset_name,\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "\n",
    "normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "normal_entries = {}\n",
    "for i, feature_name in enumerate(normal_df_train.columns.to_list()):\n",
    "    normal_entries[feature_name] = [json.loads(doc.replace(\"'\", \"\\\"\"))[i] for doc in normal_documents]\n",
    "\n",
    "attack_entries = {}\n",
    "for i, feature_name in enumerate(attack_df_train.columns.to_list()):\n",
    "    attack_entries[feature_name] = [json.loads(doc.replace(\"'\", \"\\\"\"))[i] for doc in attack_documents]\n",
    "\n",
    "# prompt_text = prompt.invoke({\n",
    "#     \"normal_entries\": json.dumps(normal_entries),\n",
    "#     \"attack_entries\": json.dumps(attack_entries)\n",
    "# }).text\n",
    "\n",
    "# print(prompt_text)\n",
    "\n",
    "completion = chain.invoke({\n",
    "    \"normal_entries\": json.dumps(normal_entries),\n",
    "    \"attack_entries\": json.dumps(attack_entries)\n",
    "})\n",
    "\n",
    "print(completion.content)\n",
    "\n",
    "id = str(uuid.uuid4())\n",
    "with open(f\"results/llm/generated-rules-{sample_size}-llm-{model_name}.txt\", \"a\") as f:\n",
    "    f.write(f\"{id}\\n\")\n",
    "    f.write(f\"{completion.content}\\n\")\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "num_tokens_prompt = len(encoding.encode(prompt.invoke({\"normal_entries\": json.dumps(normal_entries),\"attack_entries\": json.dumps(attack_entries)}).text))\n",
    "num_tokens_completion = len(encoding.encode(str(completion.content)))\n",
    "\n",
    "print(f\"Prompt tokens: {num_tokens_prompt}\")\n",
    "print(f\"Completion tokens: {num_tokens_completion}\")\n",
    "print(f\"Total tokens: {num_tokens_prompt + num_tokens_completion}\")\n",
    "print(f\"Percentage of tokens used: {(num_tokens_prompt + num_tokens_completion) / 128000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting normal entries...: 100%|███████████████████████████| 1000/1000 [00:00<00:00, 5703.70it/s]\n",
      "Predicting attack entries...: 100%|███████████████████████████| 1000/1000 [00:00<00:00, 6965.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack     0.9369    0.9650    0.9507      1000\n",
      "      normal     0.9639    0.9350    0.9492      1000\n",
      "\n",
      "    accuracy                         0.9500      2000\n",
      "   macro avg     0.9504    0.9500    0.9500      2000\n",
      "weighted avg     0.9504    0.9500    0.9500      2000\n",
      "\n",
      "[[965  35]\n",
      " [ 65 935]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Evaluate generated rules\n",
    "################################################################################\n",
    "\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = {\"normal\": normal_df_test, \"attack\": attack_df_test}\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for attack_type, dataset in datasets.items():\n",
    "    test_set_size = dataset.shape[0]\n",
    "    for i in tqdm(range(test_set_size), ncols=100, desc=f\"Predicting {attack_type} entries...\"):\n",
    "        predicted_attack_types = []\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['flow_duration'] < 1 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Header_Length'] < 1000 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Rate'] < 10 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['ack_flag_number'] == 0 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['HTTPS'] == 0 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Max'] == 54 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Protocol Type'] == 6 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Duration'] == 64 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['syn_flag_number'] > 0 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Srate'] < 10 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Drate'] < 10 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['ack_count'] < 1 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Tot sum']< 1000 else \"normal\")\n",
    "        y_true.append(attack_type)\n",
    "        y_pred.append(mode(predicted_attack_types))\n",
    "\n",
    "c_report = classification_report(y_true, y_pred, digits=4)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/llm/result-llm-{sample_size}-2.txt\", \"a\") as f:\n",
    "    f.write(f\"{id}\\n\")\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\\n\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Prompt Template\n",
    "################################################################################\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "system_message = (\"system\",\n",
    "\"\"\"\n",
    "You are a good data analyst.\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Your task is to generate {k} simple and deterministic rules for top {k} important features to filter attack entries.\n",
    "Supported operators are '==', '!=', '>', '<', '>=', '<='.\n",
    "Generate exactly {k} rules to filter attack entries and make a tool call for each rule.\n",
    "\"\"\"\n",
    ")\n",
    "human_message = (\"user\",\n",
    "\"\"\"\n",
    "Analyze the following network data and generate rules for the top 5 important features to filter attack entries.\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    human_message,\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "# Invoke prompt\n",
    "# prompt.invoke({\"k\": 5, \"normal_entries\": normal_entries, \"attack_entries\": attack_entries, \"msgs\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Tool\n",
    "################################################################################\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import operator\n",
    "from typing import Annotated\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "show_progress = True\n",
    "operations = {'<': operator.lt, '>': operator.gt, '==': operator.eq, '<=': operator.le, '>=': operator.ge, '!=': operator.ne}\n",
    "\n",
    "@tool\n",
    "def evaluate_rule(\n",
    "    feature_name: Annotated[str, \"Feature name\"],\n",
    "    value: Annotated[str, \"Value\"], \n",
    "    op: Annotated[str, \"Operator\"]\n",
    ") -> bool:\n",
    "    \"\"\"Evaluate the rule and return the macro f1-score.\"\"\"\n",
    "    try:\n",
    "        value = float(value)\n",
    "    except ValueError:\n",
    "        value\n",
    "    datasets = {\"normal\": normal_df_train, \"attack\": attack_df_train}\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    if op in operations:\n",
    "        for attack_type, dataset in datasets.items():\n",
    "            test_set_size = dataset.shape[0]\n",
    "            for i in tqdm(range(test_set_size), ncols=100, desc=f\"Predicting {attack_type} entries...\", disable=not show_progress):\n",
    "                y_true.append(attack_type)\n",
    "                y_pred.append(\"attack\" if operations[op](dataset.iloc[i][feature_name], value) else \"normal\")\n",
    "        c_report = classification_report(y_true, y_pred, digits=4, output_dict=True)\n",
    "        return c_report['macro avg']['f1-score']\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported operator: {op}\")\n",
    "\n",
    "# Invoke tool\n",
    "# print(evaluate_rule.invoke({\"feature_name\": \"flow_duration\", \"value\": \"1\", \"op\": \"<\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# LLM\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "llm = ChatOpenAI(model=model_name, temperature=0.1)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "# llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.0)\n",
    "# model_name = \"claude-3-opus-20240229\"\n",
    "# llm = ChatAnthropic(model=model_name, temperature=0.0)\n",
    "\n",
    "llm_with_tool = llm.bind_tools([evaluate_rule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Vector Store\n",
    "################################################################################\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "train_set_size = sample_size\n",
    "n_results = 10\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=dataset_name,\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "\n",
    "normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=n_results)['documents'][0]\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=n_results)['documents'][0]\n",
    "\n",
    "normal_entries_dict = {}\n",
    "for i, feature_name in enumerate(normal_df_train.columns.to_list()):\n",
    "    normal_entries_dict[feature_name] = [json.loads(doc.replace(\"'\", \"\\\"\"))[i] for doc in normal_documents]\n",
    "\n",
    "attack_entries_dict = {}\n",
    "for i, feature_name in enumerate(attack_df_train.columns.to_list()):\n",
    "    attack_entries_dict[feature_name] = [json.loads(doc.replace(\"'\", \"\\\"\"))[i] for doc in attack_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 1 Current mean f1-score: 0.6411302584590476 Token usage: {'completion_tokens': 732, 'prompt_tokens': 5450, 'total_tokens': 6182}\n",
      "Round: 2 Current mean f1-score: 0.6827872672551862 Token usage: {'completion_tokens': 389, 'prompt_tokens': 6357, 'total_tokens': 6746}\n",
      "Round: 3 Current mean f1-score: 0.6616197951100998 Token usage: {'completion_tokens': 349, 'prompt_tokens': 6920, 'total_tokens': 7269}\n",
      "Round: 4 Current mean f1-score: 0.6827898256690854 Token usage: {'completion_tokens': 317, 'prompt_tokens': 7443, 'total_tokens': 7760}\n",
      "Round: 5 Current mean f1-score: 0.6827898256690854 Token usage: {'completion_tokens': 314, 'prompt_tokens': 7934, 'total_tokens': 8248}\n",
      "[0.6411302584590476, 0.6827872672551862, 0.6616197951100998, 0.6827898256690854, 0.6827898256690854]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Chain\n",
    "################################################################################\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chain = prompt | llm_with_tool\n",
    "\n",
    "n_repetitions = 5\n",
    "context_window = 128000\n",
    "show_progress = False\n",
    "\n",
    "def get_initial_state():\n",
    "  n = 0\n",
    "  k = 5\n",
    "  mean_f1s = 0\n",
    "  max_f1s = 0\n",
    "  n_max = 0\n",
    "  token_usage = {}\n",
    "  normal_entries = json.dumps(normal_entries_dict)\n",
    "  attack_entries = json.dumps(attack_entries_dict)\n",
    "  msgs = []\n",
    "  return locals()\n",
    "\n",
    "state = get_initial_state()\n",
    "train_f1_scores = []\n",
    "while state[\"n\"] < n_repetitions:\n",
    "    ai_msg = chain.invoke(state)\n",
    "    tool_msgs = []\n",
    "    for tool_call in ai_msg.tool_calls:\n",
    "        tool_msg = evaluate_rule.invoke(tool_call)\n",
    "        tool_msgs.append(tool_msg)\n",
    "    state[\"mean_f1s\"] = sum(float(msg.content) for msg in tool_msgs) / len(tool_msgs)\n",
    "    human_msg = HumanMessage(f\"The current mean f1-score for the generated rules is {state['mean_f1s']}. \"\n",
    "                             \"If this mean f1-score is greater than the previous rounds, keep the better performing \"\n",
    "                             \"rules and revise or replace only the underperforming ones (those with a score less than mean). \"\n",
    "                             \"Otherwise, revise or replace any rules that have a score less than mean. \"\n",
    "                             f\"Based on the feedback, generate exactly {state['k']} rules to filter attack entries and \"\n",
    "                             \"make a tool call for each rule, ensuring that a tool call is made for every entry every time.\")    \n",
    "    state[\"n\"] += 1\n",
    "    state[\"msgs\"].extend([ai_msg, *tool_msgs, human_msg])\n",
    "    train_f1_scores.append(state[\"mean_f1s\"])\n",
    "    state[\"max_f1s\"] = state[\"mean_f1s\"] if state[\"mean_f1s\"] > state[\"max_f1s\"] else state[\"max_f1s\"]\n",
    "    state[\"n_max\"] = state[\"n\"] if state[\"mean_f1s\"] > state[\"max_f1s\"] else state[\"n_max\"]\n",
    "    state[\"token_usage\"] = {key: ai_msg.response_metadata[\"token_usage\"][key] for key in [\"completion_tokens\", \"prompt_tokens\", \"total_tokens\"]} \n",
    "    print(\"Round:\", state[\"n\"], \"Current mean f1-score:\", state[\"mean_f1s\"], \"Token usage:\", state[\"token_usage\"])\n",
    "print(train_f1_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack if flow_duration < 1 else normal\n",
      "attack if Header_Length < 1000 else normal\n",
      "attack if Duration == 64 else normal\n",
      "attack if syn_flag_number == 1 else normal\n",
      "attack if ack_flag_number == 0 else normal\n",
      "0.8006668027669441\n",
      "attack if flow_duration < 1 else normal\n",
      "attack if Header_Length < 1000 else normal\n",
      "attack if Duration <= 70 else normal\n",
      "attack if syn_flag_number >= 0.5 else normal\n",
      "attack if ack_flag_number == 0 else normal\n",
      "0.9019476275688372\n",
      "attack if flow_duration < 1 else normal\n",
      "attack if Header_Length < 1000 else normal\n",
      "attack if Duration <= 70 else normal\n",
      "attack if psh_flag_number == 1 else normal\n",
      "attack if ack_flag_number == 0 else normal\n",
      "0.9045155019750267\n",
      "attack if flow_duration < 1 else normal\n",
      "attack if Header_Length < 1000 else normal\n",
      "attack if Duration <= 70 else normal\n",
      "attack if Rate > 50 else normal\n",
      "attack if ack_flag_number == 0 else normal\n",
      "0.9426194924959315\n",
      "attack if flow_duration < 1 else normal\n",
      "attack if Header_Length < 1000 else normal\n",
      "attack if Duration <= 70 else normal\n",
      "attack if Srate > 50 else normal\n",
      "attack if ack_flag_number == 0 else normal\n",
      "0.9426194924959315\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Evaluate generated rules\n",
    "################################################################################\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import operator\n",
    "from statistics import mode\n",
    "\n",
    "operations = {'<': operator.lt, '>': operator.gt, '==': operator.eq, '<=': operator.le, '>=': operator.ge, '!=': operator.ne}\n",
    "\n",
    "def evaluate_rules(tool_calls):\n",
    "    datasets = {\"normal\": normal_df_test, \"attack\": attack_df_test}\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for attack_type, dataset in datasets.items():\n",
    "        test_set_size = dataset.shape[0]\n",
    "        for i in tqdm(range(test_set_size), ncols=100, desc=f\"Predicting {attack_type} entries...\", disable=not show_progress):\n",
    "            predicted_attack_types = []\n",
    "            for tool_call in tool_calls:\n",
    "                args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "                op = args[\"op\"]\n",
    "                feature_name = args[\"feature_name\"]\n",
    "                value = args[\"value\"]\n",
    "                try:\n",
    "                    value = float(value)\n",
    "                except ValueError:\n",
    "                    value\n",
    "                predicted_attack_types.append(\"attack\" if operations[op](dataset.iloc[i][feature_name], value) else \"normal\")\n",
    "            y_true.append(attack_type)\n",
    "            y_pred.append(mode(predicted_attack_types))\n",
    "    c_report = classification_report(y_true, y_pred, digits=4, output_dict=True)\n",
    "    c_matrix = confusion_matrix(y_true, y_pred)\n",
    "    # print(c_report)\n",
    "    # print(c_matrix)\n",
    "    return c_report\n",
    "\n",
    "# tool_calls = state[\"msgs\"][-7].additional_kwargs[\"tool_calls\"]\n",
    "# for tool_call in tool_calls:\n",
    "#     rule = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "#     print(\"attack if\", rule[\"feature_name\"], rule[\"op\"], rule[\"value\"], \"else normal\")\n",
    "\n",
    "# evaluate_rules(tool_calls)\n",
    "\n",
    "# test_f1_scores = []\n",
    "# for i in range(20, 0, -1):\n",
    "#     index = -7 * i\n",
    "#     tool_calls = state[\"msgs\"][index].additional_kwargs[\"tool_calls\"]\n",
    "#     for tool_call in tool_calls:\n",
    "#         rule = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "#     test_f1_scores.append(evaluate_rules(tool_calls)['macro avg']['f1-score'])\n",
    "\n",
    "# print(test_f1_scores)\n",
    "\n",
    "for i in range(len(state[\"msgs\"])):\n",
    "    if state[\"msgs\"][i].type != \"ai\":\n",
    "        continue\n",
    "    tool_calls = state[\"msgs\"][i].additional_kwargs[\"tool_calls\"]\n",
    "    for tool_call in tool_calls:\n",
    "        rule = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "        print(\"attack if\", rule[\"feature_name\"], rule[\"op\"], rule[\"value\"], \"else normal\")\n",
    "    c_report = evaluate_rules(tool_calls)\n",
    "    print(c_report[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT time taken: 0.0002441995024681091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack     0.9960    0.9961    0.9961     17840\n",
      "      normal     0.9680    0.9671    0.9676      2160\n",
      "\n",
      "    accuracy                         0.9930     20000\n",
      "   macro avg     0.9820    0.9816    0.9818     20000\n",
      "weighted avg     0.9930    0.9930    0.9930     20000\n",
      "\n",
      "[[17771    69]\n",
      " [   71  2089]]\n",
      "\n",
      "\n",
      "RF time taken: 0.0034858589172363282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack     0.9991    0.9954    0.9972     17840\n",
      "      normal     0.9632    0.9926    0.9777      2160\n",
      "\n",
      "    accuracy                         0.9951     20000\n",
      "   macro avg     0.9811    0.9940    0.9875     20000\n",
      "weighted avg     0.9952    0.9951    0.9951     20000\n",
      "\n",
      "[[17758    82]\n",
      " [   16  2144]]\n",
      "\n",
      "\n",
      "LLM time taken: 0.0001223496913909912\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack     0.9973    0.9760    0.9865     17840\n",
      "      normal     0.8315    0.9778    0.8987      2160\n",
      "\n",
      "    accuracy                         0.9762     20000\n",
      "   macro avg     0.9144    0.9769    0.9426     20000\n",
      "weighted avg     0.9793    0.9762    0.9770     20000\n",
      "\n",
      "[[17412   428]\n",
      " [   48  2112]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Evaluate generated rules for efficiency\n",
    "################################################################################\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tabulate import tabulate\n",
    "from statistics import mode\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sample_size = 100000\n",
    "\n",
    "# Load dateset\n",
    "df = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2.csv')\n",
    "\n",
    "# This dataset does not need categorical encoding as all features are numerical\n",
    "# except for the label.\n",
    "\n",
    "# Split dataset according to attack type\n",
    "normal_df = df[df['label'] == 'BenignTraffic']\n",
    "attack_df = df[df['label'] != 'BenignTraffic']\n",
    "normal_df.loc[:, 'label'] = 'normal'\n",
    "attack_df.loc[:, 'label'] = 'attack'\n",
    "\n",
    "# Split dataset into training and test set\n",
    "normal_df_train = normal_df.sample(frac=0.8, random_state=42)\n",
    "normal_df_test = normal_df.drop(normal_df_train.index)\n",
    "attack_df_train = attack_df.sample(frac=0.8, random_state=42)\n",
    "attack_df_test = attack_df.drop(attack_df_train.index)\n",
    "\n",
    "X_train = pd.concat([normal_df_train, attack_df_train]).drop(columns=['label'])\n",
    "y_train = pd.concat([normal_df_train, attack_df_train])['label']\n",
    "X_test = pd.concat([normal_df_test, attack_df_test]).drop(columns=['label'])\n",
    "y_test = pd.concat([normal_df_test, attack_df_test])['label']\n",
    "\n",
    "# Create instances of ML models\n",
    "model_dt = DecisionTreeClassifier()\n",
    "model_rf = RandomForestClassifier()\n",
    "\n",
    "# Fit the models to the training data\n",
    "model_dt.fit(X_train, y_train)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "\n",
    "elapsed_times_dt = []\n",
    "elapsed_times_rf = []\n",
    "elapsed_times_llm = []\n",
    "y_pred_dt = []\n",
    "y_pred_rf = []\n",
    "y_pred_llm = []\n",
    "for i in range(len(X_test)):\n",
    "    # Predict using DT\n",
    "    start = time.time()\n",
    "    y_pred_dt.append(model_dt.predict([X_test.iloc[i]]))\n",
    "    end = time.time()\n",
    "    elapsed_times_dt.append(end - start)\n",
    "\n",
    "    # Predict using RF\n",
    "    start = time.time()\n",
    "    y_pred_rf.append(model_rf.predict([X_test.iloc[i]]))\n",
    "    end = time.time()\n",
    "    elapsed_times_rf.append(end - start)\n",
    "    \n",
    "    # Predict using LLM\n",
    "    start = time.time()\n",
    "    row = X_test.iloc[i]\n",
    "    conditions = [\n",
    "        row['flow_duration'] < 1,\n",
    "        row['Header_Length'] < 1000,\n",
    "        row['Duration'] <= 70,\n",
    "        row['Srate'] > 50,\n",
    "        row['ack_flag_number'] == 0\n",
    "    ]\n",
    "    predicted_attack_types = [\"attack\" if condition else \"normal\" for condition in conditions]\n",
    "    y_pred_llm.append(mode(predicted_attack_types))\n",
    "    end = time.time()\n",
    "    elapsed_times_llm.append(end - start)\n",
    "\n",
    "print(f\"DT time taken: {sum(elapsed_times_dt)/len(X_test)}\")\n",
    "print(classification_report(y_true, y_pred_dt, digits=4, output_dict=False))\n",
    "print(confusion_matrix(y_true, y_pred_dt))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"RF time taken: {sum(elapsed_times_rf)/len(X_test)}\")\n",
    "print(classification_report(y_true, y_pred_rf, digits=4, output_dict=False))\n",
    "print(confusion_matrix(y_true, y_pred_rf))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"LLM time taken: {sum(elapsed_times_llm)/len(X_test)}\\n\")\n",
    "print(classification_report(y_true, y_pred_llm, digits=4, output_dict=False))\n",
    "print(confusion_matrix(y_true, y_pred_llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function to use llm to explain the generated rules\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "system_message = (\"system\",\n",
    "\"\"\"\n",
    "You are a good data analyst.\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Your task is to generate {k} simple and deterministic rules for top {k} important features to filter attack entries.\n",
    "Supported operators are '==', '!=', '>', '<', '>=', '<='.\n",
    "Generate exactly {k} rules to filter attack entries and make a tool call for each rule.\n",
    "\"\"\"\n",
    ")\n",
    "human_message = (\"user\",\n",
    "\"\"\"\n",
    "Analyze the following network data and generate rules for the top 5 important features to filter attack entries.\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    human_message,\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "llm = ChatOpenAI(model=model_name, temperature=0.0)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "# llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.0)\n",
    "# model_name = \"claude-3-opus-20240229\"\n",
    "# llm = ChatAnthropic(model=model_name, temperature=0.0)\n",
    "\n",
    "llm_with_tool = llm.bind_tools([evaluate_rule])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (518245297.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[68], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    from langchain_core.prompts import\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Generate Rules with Feedback Loop\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import tiktoken     # https://github.com/openai/tiktoken\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You were provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "You were asked to carefully analyze the differences between normal and attack entries by comparing corresponding fields to \n",
    "generate 5 simple and deterministic rules for the top 5 important features to filter an entry as either normal or attack.\n",
    "The rules you generated were evaluated individually against a test set of network data entries, and the following F1-scores were obtained:\n",
    "\n",
    "F1-scores 1st round:\n",
    "```\n",
    "\"flow_duration\": \"if flow_duration < 1 then attack else normal\" --> 0.9064\n",
    "\"Header_Length\": \"if Header_Length <= 1000 then attack else normal\" --> 0.8295\n",
    "\"ack_flag_number\": \"if ack_flag_number == 0 then attack else normal\" --> 0.8690\n",
    "\"HTTPS\": \"if HTTPS == 0 then attack else normal\" --> 0.8160\n",
    "\"Max\": \"if Max == 54.0 then attack else normal\" --> 0.6968\n",
    "\n",
    "Overall F1-score --> 0.9735\n",
    "```\n",
    "\n",
    "F1-scores 2nd round:\n",
    "```\n",
    "\"flow_duration\": \"if flow_duration < 1 then attack else normal\" --> 0.9064\n",
    "\"Header_Length\": \"if Header_Length <= 100 then attack else normal\" --> 0.8265\n",
    "\"ack_flag_number\": \"if ack_flag_number == 0 then attack else normal\" --> 0.8690\n",
    "\"HTTPS\": \"if HTTPS == 0 then attack else normal\" --> 0.8160\n",
    "\"Duration\": \"if Duration <= 70 then attack else normal\" --> 0.3329\n",
    "\n",
    "Overall F1-score --> 0.9332\n",
    "```\n",
    "\n",
    "F1-scores 3rd round:\n",
    "```\n",
    "\"flow_duration\": \"if flow_duration < 1 then attack else normal\" --> 0.9064\n",
    "\"Header_Length\": \"if Header_Length <= 100 then attack else normal\" --> 0.8265\n",
    "\"ack_flag_number\": \"if ack_flag_number == 0 then attack else normal\" --> 0.8690\n",
    "\"HTTPS\": \"if HTTPS == 0 then attack else normal\" --> 0.8160\n",
    "\"AVG\": \"if AVG <= 60 then attack else normal\" --> 0.9388\n",
    "\n",
    "Overall F1-score --> 0.9700\n",
    "```\n",
    "\n",
    "Based on the feedback provided, drop underperforming rules that has the least f1-score.\n",
    "Generate new rules to revise the rules to improve the F1-scores.\n",
    "Output only in the JSON format with the structure: \n",
    "{{'feature1': 'rule', 'feature2': 'rule', ..., 'feature5': 'rule'}}.\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"),\n",
    "    (\"user\", \"{}\")\n",
    "])\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "model_name = \"gpt-4o\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "# llm = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "# model_name = \"claude-3-opus-20240229\"\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"cic-iot\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "\n",
    "normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "normal_entries = {}\n",
    "for i, feature_name in enumerate(normal_df_train.columns.to_list()):\n",
    "    normal_entries[feature_name] = [json.loads(doc)[i] for doc in normal_documents]\n",
    "\n",
    "attack_entries = {}\n",
    "for i, feature_name in enumerate(attack_df_train.columns.to_list()):\n",
    "    attack_entries[feature_name] = [json.loads(doc)[i] for doc in attack_documents]\n",
    "\n",
    "# print(prompt.invoke({\n",
    "#     \"normal_entries\": json.dumps(normal_entries),\n",
    "#     \"attack_entries\": json.dumps(attack_entries)\n",
    "# }).text)\n",
    "\n",
    "completion = chain.invoke({\n",
    "    \"normal_entries\": json.dumps(normal_entries),\n",
    "    \"attack_entries\": json.dumps(attack_entries)\n",
    "})\n",
    "\n",
    "print(completion.content)\n",
    "\n",
    "with open(f\"results/llm/generated-rules-{sample_size}-llm-{model_name}.txt\", \"a\") as f:\n",
    "    f.write(completion.content)\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "num_tokens_prompt = len(encoding.encode(prompt.invoke({\"normal_entries\": json.dumps(normal_entries),\"attack_entries\": json.dumps(attack_entries)}).text))\n",
    "num_tokens_completion = len(encoding.encode(str(completion.content)))\n",
    "\n",
    "print(f\"Prompt tokens: {num_tokens_prompt}\")\n",
    "print(f\"Completion tokens: {num_tokens_completion}\")\n",
    "print(f\"Total tokens: {num_tokens_prompt + num_tokens_completion}\")\n",
    "print(f\"Percentage of tokens used: {(num_tokens_prompt + num_tokens_completion) / 128000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Generate Rules\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Generate 5 simple and deterministic rules for top 5 important features to filter an entry as either normal or attack. \n",
    "Output only in the JSON format with the structure: \n",
    "{{'feature1': 'rule', 'feature2': 'rule', ..., 'feature5': 'rule'}}.\n",
    "\n",
    "Feature Names:\n",
    "```{feature_names}```\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"feature_names\", \"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "model_name = \"gpt-4o\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "# llm = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "# model_name = \"claude-3-opus-20240229\"\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"cic-iot\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "\n",
    "normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "completion = chain.invoke({\n",
    "    \"feature_names\": normal_df_train.columns.to_list(),\n",
    "    \"normal_entries\": \",\\n\".join([f\"{doc} --> normal\" for doc in normal_documents]),\n",
    "    \"attack_entries\": \",\\n\".join([f\"{doc} --> attack\" for doc in attack_documents])\n",
    "    })\n",
    "\n",
    "print(completion.content)\n",
    "\n",
    "with open(f\"results/generated-rules-{sample_size}-llm-{model_name}.txt\", \"a\") as f:\n",
    "    f.write(completion.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Evaluate generated rules\n",
    "################################################################################\n",
    "\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = {\"normal\": normal_df_test, \"attack\": attack_df_test}\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for attack_type, dataset in datasets.items():\n",
    "    test_set_size = dataset.shape[0]\n",
    "    for i in tqdm(range(test_set_size), ncols=100, desc=f\"Predicting {attack_type} entries...\"):\n",
    "        predicted_attack_types = []\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['flow_duration'] < 1 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Header_Length'] < 100 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Duration'] == 64 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Rate'] < 10 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['ack_flag_number'] == 0 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['syn_flag_number'] > 0 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Srate'] < 10 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Drate'] < 10 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['ack_count'] < 1 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Tot sum']< 1000 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Max'] < 100 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Protocol Type'] == 6 else \"normal\")\n",
    "        y_true.append(attack_type)\n",
    "        y_pred.append(mode(predicted_attack_types))\n",
    "\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-llm-{sample_size}-2.txt\", \"a\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Get a Summary\n",
    "################################################################################\n",
    "\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tiktoken     # https://github.com/openai/tiktoken\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "Given normal and attack network data entries, output human understandable small summary on \n",
    "how attack and normal entries can be simply separated.\n",
    "\n",
    "Feature Names:\n",
    "```{feature_names}```\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"feature_names\", \"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"cic-iot\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 5})\n",
    "\n",
    "# retriever = vector_store.as_retriever(\n",
    "#     search_type=\"mmr\", \n",
    "#     search_kwargs={\"k\": 10, \"fetch_k\": 10})\n",
    "# normal_documents = retriever.invoke(str(normal_df_test.iloc[0].to_list()), filter={\"source\": \"cic-iot\", \"label\": \"normal\"})\n",
    "# attack_documents = retriever.invoke(str(attack_df_test.iloc[0].to_list()), filter={\"source\": \"cic-iot\", \"label\": \"attack\"})\n",
    "# completion = chain.invoke({\n",
    "#     \"feature_names\": normal_df_train.columns.to_list(),\n",
    "#     \"normal_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in normal_documents]),\n",
    "#     \"attack_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in attack_documents])\n",
    "#     })\n",
    "# print(completion)\n",
    "\n",
    "normal_documents = retriever.invoke(str(normal_df_test.iloc[0].to_list()), filter={\"source\": \"cic-iot\", \"label\": \"normal\"})\n",
    "attack_documents = retriever.invoke(str(attack_df_test.iloc[0].to_list()), filter={\"source\": \"cic-iot\", \"label\": \"attack\"})\n",
    "completion = chain.invoke({\n",
    "    \"feature_names\": normal_df_train.columns.to_list(),\n",
    "    \"normal_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in normal_documents]),\n",
    "    \"attack_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in attack_documents])\n",
    "    })\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S4025371\\OneDrive - RMIT University\\Repositories\\iot-llm\\.conda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To generate a decision tree that can differentiate between normal and attack entries, we need to analyze the provided data and identify the most significant features that distinguish the two categories. Let's start by comparing the features of normal and attack entries.\n",
      "\n",
      "### Feature Analysis\n",
      "\n",
      "1. **Flow Duration**:\n",
      "   - Normal: Ranges from approximately 41 to 82.\n",
      "   - Attack: Ranges from 0 to 0.48.\n",
      "   - **Observation**: Attack entries have significantly lower flow durations.\n",
      "\n",
      "2. **Header Length**:\n",
      "   - Normal: Ranges from approximately 337,755 to 4,609,454.\n",
      "   - Attack: Ranges from 54 to 166.\n",
      "   - **Observation**: Attack entries have much smaller header lengths.\n",
      "\n",
      "3. **Protocol Type**:\n",
      "   - Normal: Mostly around 6, with some variations.\n",
      "   - Attack: Consistently 6.\n",
      "   - **Observation**: Not a distinguishing feature.\n",
      "\n",
      "4. **Rate**:\n",
      "   - Normal: Ranges from approximately 19.9 to 83.9.\n",
      "   - Attack: Ranges from approximately 2.38 to 178.43.\n",
      "   - **Observation**: Attack entries have a wider range, with some very low and some very high rates.\n",
      "\n",
      "5. **Flag Numbers (fin, syn, rst, psh, ack)**:\n",
      "   - Normal: Mostly 0, with some 1s in `psh_flag_number`.\n",
      "   - Attack: More variability, with some 1s in `fin_flag_number`, `syn_flag_number`, `rst_flag_number`, and `psh_flag_number`.\n",
      "   - **Observation**: Attack entries show more variability in flag numbers.\n",
      "\n",
      "6. **Tot Sum, Min, Max, AVG, Std, Tot Size**:\n",
      "   - Normal: Higher values and more variability.\n",
      "   - Attack: Consistently low values.\n",
      "   - **Observation**: Attack entries have consistently lower values across these metrics.\n",
      "\n",
      "7. **IAT (Inter-Arrival Time)**:\n",
      "   - Normal: Ranges from very small values to very large.\n",
      "   - Attack: Consistently large values.\n",
      "   - **Observation**: Attack entries have consistently large IAT values.\n",
      "\n",
      "8. **Weight**:\n",
      "   - Normal: Ranges from 38.5 to 244.6.\n",
      "   - Attack: Consistently 141.55.\n",
      "   - **Observation**: Attack entries have a consistent weight.\n",
      "\n",
      "### Top 5 Important Features\n",
      "\n",
      "Based on the analysis, the top 5 features that can help distinguish between normal and attack entries are:\n",
      "\n",
      "1. **Flow Duration**: Significant difference in values.\n",
      "2. **Header Length**: Significant difference in values.\n",
      "3. **Rate**: Wider range in attack entries.\n",
      "4. **IAT**: Consistently large in attack entries.\n",
      "5. **Tot Sum**: Consistently lower in attack entries.\n",
      "\n",
      "### Decision Tree\n",
      "\n",
      "Using these features, we can construct a simple decision tree:\n",
      "\n",
      "1. **Flow Duration**:\n",
      "   - If `flow_duration` < 1, classify as **Attack**.\n",
      "   - Else, proceed to the next feature.\n",
      "\n",
      "2. **Header Length**:\n",
      "   - If `Header_Length` < 200, classify as **Attack**.\n",
      "   - Else, proceed to the next feature.\n",
      "\n",
      "3. **Rate**:\n",
      "   - If `Rate` < 5 or `Rate` > 150, classify as **Attack**.\n",
      "   - Else, proceed to the next feature.\n",
      "\n",
      "4. **IAT**:\n",
      "   - If `IAT` > 1e7, classify as **Attack**.\n",
      "   - Else, proceed to the next feature.\n",
      "\n",
      "5. **Tot Sum**:\n",
      "   - If `Tot sum` < 600, classify as **Attack**.\n",
      "   - Else, classify as **Normal**.\n",
      "\n",
      "This decision tree uses the most significant features to differentiate between normal and attack entries effectively.\n",
      "Prompt tokens: 5318\n",
      "Completion tokens: 802\n",
      "Total tokens: 6120\n",
      "Percentage of tokens used: 0.0478125\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Generate Decision Tree\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import uuid\n",
    "import tiktoken     # https://github.com/openai/tiktoken\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Generate a decision tree with top 5 important features as nodes to filter an entry as either normal or attack. \n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "model_name = \"gpt-4o\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "# llm = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "# model_name = \"claude-3-opus-20240229\"\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=dataset_name,\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "\n",
    "normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "normal_entries = {}\n",
    "for i, feature_name in enumerate(normal_df_train.columns.to_list()):\n",
    "    normal_entries[feature_name] = [json.loads(doc.replace(\"'\", \"\\\"\"))[i] for doc in normal_documents]\n",
    "\n",
    "attack_entries = {}\n",
    "for i, feature_name in enumerate(attack_df_train.columns.to_list()):\n",
    "    attack_entries[feature_name] = [json.loads(doc.replace(\"'\", \"\\\"\"))[i] for doc in attack_documents]\n",
    "\n",
    "# prompt_text = prompt.invoke({\n",
    "#     \"normal_entries\": json.dumps(normal_entries),\n",
    "#     \"attack_entries\": json.dumps(attack_entries)\n",
    "# }).text\n",
    "\n",
    "# print(prompt_text)\n",
    "\n",
    "completion = chain.invoke({\n",
    "    \"normal_entries\": json.dumps(normal_entries),\n",
    "    \"attack_entries\": json.dumps(attack_entries)\n",
    "})\n",
    "\n",
    "print(completion.content)\n",
    "\n",
    "id = str(uuid.uuid4())\n",
    "with open(f\"results/llm/generated-rules-{sample_size}-llm-{model_name}.txt\", \"a\") as f:\n",
    "    f.write(f\"{id}\\n\")\n",
    "    f.write(f\"{completion.content}\\n\")\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "num_tokens_prompt = len(encoding.encode(prompt.invoke({\"normal_entries\": json.dumps(normal_entries),\"attack_entries\": json.dumps(attack_entries)}).text))\n",
    "num_tokens_completion = len(encoding.encode(str(completion.content)))\n",
    "\n",
    "print(f\"Prompt tokens: {num_tokens_prompt}\")\n",
    "print(f\"Completion tokens: {num_tokens_completion}\")\n",
    "print(f\"Total tokens: {num_tokens_prompt + num_tokens_completion}\")\n",
    "print(f\"Percentage of tokens used: {(num_tokens_prompt + num_tokens_completion) / 128000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting normal entries...: 100%|██████████████████████████| 1000/1000 [00:00<00:00, 14100.92it/s]\n",
      "Predicting attack entries...: 100%|██████████████████████████| 1000/1000 [00:00<00:00, 23512.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack     0.6172    0.9980    0.7627      1000\n",
      "      normal     0.9948    0.3810    0.5510      1000\n",
      "\n",
      "    accuracy                         0.6895      2000\n",
      "   macro avg     0.8060    0.6895    0.6568      2000\n",
      "weighted avg     0.8060    0.6895    0.6568      2000\n",
      "\n",
      "[[998   2]\n",
      " [619 381]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Evaluate generated rules\n",
    "################################################################################\n",
    "\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "def classify_traffic(record):\n",
    "    # Step 1: Check Flow Duration\n",
    "    if record[\"flow_duration\"] < 1:\n",
    "        return \"attack\"\n",
    "    \n",
    "    # Step 2: Check Header Length\n",
    "    if record[\"Header_Length\"] < 200:\n",
    "        return \"attack\"\n",
    "    \n",
    "    # Step 3: Check Rate\n",
    "    if record[\"Rate\"] < 10:\n",
    "        return \"attack\"\n",
    "    \n",
    "    # Step 4: Check IAT\n",
    "    if record[\"ack_flag_number\"] == 0:\n",
    "        return \"attack\"\n",
    "    \n",
    "    # Step 5: Check Tot Sum\n",
    "    if record[\"Weight\"] > 100:\n",
    "        return \"attack\"\n",
    "    \n",
    "    return \"normal\"\n",
    "\n",
    "datasets = {\"normal\": normal_df_test, \"attack\": attack_df_test}\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for attack_type, dataset in datasets.items():\n",
    "    test_set_size = dataset.shape[0]\n",
    "    for i in tqdm(range(test_set_size), ncols=100, desc=f\"Predicting {attack_type} entries...\"):\n",
    "        y_true.append(attack_type)\n",
    "        y_pred.append(classify_traffic(dataset.iloc[i]))\n",
    "\n",
    "c_report = classification_report(y_true, y_pred, digits=4)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
