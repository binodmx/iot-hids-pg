{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+--------+\n",
      "| Atack type   |   Total |   Train |   Test |\n",
      "+==============+=========+=========+========+\n",
      "| Normal       |    5000 |    4000 |   1000 |\n",
      "+--------------+---------+---------+--------+\n",
      "| Attack       |    5000 |    4000 |   1000 |\n",
      "+--------------+---------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Load dataset and split it into training and test set\n",
    "################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "sample_size = 10000\n",
    "\n",
    "# Load dateset\n",
    "df = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2.csv')\n",
    "\n",
    "# Split dataset according to attack type\n",
    "normal_df = df[df['label'] == 'BenignTraffic']\n",
    "attack_df = df[df['label'] != 'BenignTraffic']\n",
    "\n",
    "# Drop columns\n",
    "normal_df = normal_df.drop(columns=['label'])\n",
    "attack_df = attack_df.drop(columns=['label'])\n",
    "\n",
    "# Split dataset into training and test set\n",
    "normal_df_train = normal_df.sample(frac=0.8, random_state=42)\n",
    "normal_df_test = normal_df.drop(normal_df_train.index)\n",
    "attack_df_train = attack_df.sample(frac=0.8, random_state=42)\n",
    "attack_df_test = attack_df.drop(attack_df_train.index)\n",
    "\n",
    "# Print dataset sizes in a table\n",
    "data = [\n",
    "    [\"Normal\", normal_df.shape[0], normal_df_train.shape[0], normal_df_test.shape[0]],\n",
    "    [\"Attack\", attack_df.shape[0], attack_df_train.shape[0], attack_df_test.shape[0]]\n",
    "]\n",
    "print(tabulate(data, headers=[\"Atack type\", \"Total\", \"Train\", \"Test\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Generate Feature Importance\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "import time\n",
    "import numpy as np\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Output top 5 important features that can be used to filter an entry as either normal or attack.\n",
    "Output only in the Python list structure.\n",
    "\n",
    "Feature Names:\n",
    "```{feature_names}```\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\n",
    "Example output:\n",
    "['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"feature_names\", \"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "model_name = \"gpt-4o\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"cic-iot\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "\n",
    "normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "completions = []\n",
    "for i in range(10):\n",
    "    completion = chain.invoke({\n",
    "        \"feature_names\": normal_df_train.columns.to_list(),\n",
    "        \"normal_entries\": \",\\n\".join([f\"{doc} --> normal\" for doc in normal_documents]),\n",
    "        \"attack_entries\": \",\\n\".join([f\"{doc} --> attack\" for doc in attack_documents])\n",
    "        })\n",
    "    completions.append(completion.content)\n",
    "    print(completion.content)\n",
    "    time.sleep(10)\n",
    "\n",
    "with open(f\"results/feature-importance-{sample_size}-llm-{model_name}.txt\", \"a\") as f:\n",
    "    f.write(\"\\n\".join(completions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S4025371\\OneDrive - RMIT University\\Repositories\\iot-llm\\.conda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"flow_duration\": \"if flow_duration < 1 then attack else normal\",\n",
      "  \"Header_Length\": \"if Header_Length < 100 then attack else normal\",\n",
      "  \"Rate\": \"if Rate < 10 then attack else normal\",\n",
      "  \"ack_flag_number\": \"if ack_flag_number == 0 then attack else normal\",\n",
      "  \"syn_flag_number\": \"if syn_flag_number == 1 then attack else normal\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Generate Rules\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Generate 5 simple and deterministic rules for top 5 important features to filter an entry as either normal or attack. \n",
    "Output only in the JSON format with the structure: \n",
    "{{'feature1': 'rule', 'feature2': 'rule', 'feature3': 'rule', 'feature4': 'rule', 'feature5': 'rule'}}.\n",
    "\n",
    "Feature Names:\n",
    "```{feature_names}```\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"feature_names\", \"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "model_name = \"gpt-4o\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "# llm = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "# model_name = \"claude-3-opus-20240229\"\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"cic-iot\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "\n",
    "normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=10)['documents']\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=10)['documents']\n",
    "\n",
    "completion = chain.invoke({\n",
    "    \"feature_names\": normal_df_train.columns.to_list(),\n",
    "    \"normal_entries\": \",\\n\".join([f\"{doc} --> normal\" for doc in normal_documents]),\n",
    "    \"attack_entries\": \",\\n\".join([f\"{doc} --> attack\" for doc in attack_documents])\n",
    "    })\n",
    "\n",
    "print(completion.content)\n",
    "\n",
    "with open(f\"results/generated-rules-{sample_size}-llm-{model_name}.txt\", \"a\") as f:\n",
    "    f.write(completion.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting normal entries...: 100%|███████████████████████████| 1000/1000 [00:00<00:00, 3452.14it/s]\n",
      "Predicting attack entries...: 100%|███████████████████████████| 1000/1000 [00:00<00:00, 3595.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack       1.00      1.00      1.00      1000\n",
      "      normal       1.00      1.00      1.00      1000\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "[[ 997    3]\n",
      " [   0 1000]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Evaluate generated rule\n",
    "################################################################################\n",
    "\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = {\"normal\": normal_df_test, \"attack\": attack_df_test}\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for attack_type, dataset in datasets.items():\n",
    "    test_set_size = dataset.shape[0]\n",
    "    predicted_attack_types = []\n",
    "    for i in tqdm(range(test_set_size), ncols=100, desc=f\"Predicting {attack_type} entries...\"):\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['flow_duration'] < 1 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Header_Length'] < 100 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Protocol Type'] == 6 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Rate'] < 10 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['ack_flag_number'] == 0 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['syn_flag_number'] == 1 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Duration'] == 64 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Weight'] < 200 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['HTTP'] == 0 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Tot sum'] == 525 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['Max'] == 50 else \"normal\")\n",
    "        y_true.append(attack_type)\n",
    "        y_pred.append(mode(predicted_attack_types))\n",
    "\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-llm-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Based on the provided data entries, we can see that normal entries have higher values for features such as flow duration, header length, rate, duration, and total size compared to attack entries. Additionally, normal entries have more occurrences of protocols like HTTP, HTTPS, DNS, SSH, TCP, UDP, DHCP, ARP, ICMP, and IPv.\\n\\nOn the other hand, attack entries have lower values for the mentioned features and do not have as many occurrences of the mentioned protocols. Attack entries also tend to have higher values for features like magnitude, radius, covariance, variance, and weight compared to normal entries.\\n\\nIn summary, normal entries exhibit higher network activity and a wider range of protocols, while attack entries show lower network activity and fewer protocol occurrences.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 149, 'prompt_tokens': 2787, 'total_tokens': 2936}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-93f0cf1f-2f0a-4937-95d9-da6e426ae510-0' usage_metadata={'input_tokens': 2787, 'output_tokens': 149, 'total_tokens': 2936}\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Get a Summary\n",
    "################################################################################\n",
    "\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tiktoken     # https://github.com/openai/tiktoken\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "Given normal and attack network data entries, output human understandable small summary on \n",
    "how attack and normal entries can be simply separated.\n",
    "\n",
    "Feature Names:\n",
    "```{feature_names}```\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"feature_names\", \"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"cic-iot\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 5})\n",
    "\n",
    "# retriever = vector_store.as_retriever(\n",
    "#     search_type=\"mmr\", \n",
    "#     search_kwargs={\"k\": 10, \"fetch_k\": 10})\n",
    "# normal_documents = retriever.invoke(str(normal_df_test.iloc[0].to_list()), filter={\"source\": \"cic-iot\", \"label\": \"normal\"})\n",
    "# attack_documents = retriever.invoke(str(attack_df_test.iloc[0].to_list()), filter={\"source\": \"cic-iot\", \"label\": \"attack\"})\n",
    "# completion = chain.invoke({\n",
    "#     \"feature_names\": normal_df_train.columns.to_list(),\n",
    "#     \"normal_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in normal_documents]),\n",
    "#     \"attack_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in attack_documents])\n",
    "#     })\n",
    "# print(completion)\n",
    "\n",
    "normal_documents = retriever.invoke(str(normal_df_test.iloc[0].to_list()), filter={\"source\": \"cic-iot\", \"label\": \"normal\"})\n",
    "attack_documents = retriever.invoke(str(attack_df_test.iloc[0].to_list()), filter={\"source\": \"cic-iot\", \"label\": \"attack\"})\n",
    "completion = chain.invoke({\n",
    "    \"feature_names\": normal_df_train.columns.to_list(),\n",
    "    \"normal_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in normal_documents]),\n",
    "    \"attack_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in attack_documents])\n",
    "    })\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 2780\n"
     ]
    }
   ],
   "source": [
    "# print(completion.text)\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "num_tokens = len(encoding.encode(str(completion.text)))\n",
    "print(\"Num tokens:\", num_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('llm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae27029e1f82e56f496a30980cc01ee9e9874fa5eb01d56792517e894009f269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
