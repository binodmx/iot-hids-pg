{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  8000\n",
      "Test set size:  2000\n",
      "Index(['flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate',\n",
      "       'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
      "       'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
      "       'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count',\n",
      "       'fin_count', 'urg_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'SSH',\n",
      "       'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min',\n",
      "       'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius',\n",
      "       'Covariance', 'Variance', 'Weight'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Load dataset and split it into training and test set\n",
    "################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "sample_size = 10000\n",
    "\n",
    "# Load dateset\n",
    "df = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2.csv')\n",
    "\n",
    "# Split dataset according to attack type\n",
    "normal_df = df[df['label'] == 'BenignTraffic']\n",
    "attack_df = df[df['label'] != 'BenignTraffic']\n",
    "normal_df.loc[:, 'label'] = 'normal'\n",
    "attack_df.loc[:, 'label'] = 'attack'\n",
    "\n",
    "# Split dataset into training and test set\n",
    "normal_df_train = normal_df.sample(frac=0.8, random_state=42)\n",
    "normal_df_test = normal_df.drop(normal_df_train.index)\n",
    "attack_df_train = attack_df.sample(frac=0.8, random_state=42)\n",
    "attack_df_test = attack_df.drop(attack_df_train.index)\n",
    "\n",
    "X_train = pd.concat([normal_df_train, attack_df_train]).drop(columns=['label'])\n",
    "y_train = pd.concat([normal_df_train, attack_df_train])['label']\n",
    "X_test = pd.concat([normal_df_test, attack_df_test]).drop(columns=['label'])\n",
    "y_test = pd.concat([normal_df_test, attack_df_test])['label']\n",
    "\n",
    "print(\"Training set size: \", X_train.shape[0])\n",
    "print(\"Test set size: \", X_test.shape[0])\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack       0.99      0.99      0.99      1000\n",
      "      normal       0.99      0.99      0.99      1000\n",
      "\n",
      "    accuracy                           0.99      2000\n",
      "   macro avg       0.99      0.99      0.99      2000\n",
      "weighted avg       0.99      0.99      0.99      2000\n",
      "\n",
      "[[986  14]\n",
      " [ 10 990]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Predict from Decision Tree model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-dt-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rst_count', 0.9325711880736708), ('flow_duration', 0.016766377022216086), ('IAT', 0.013764657714541733), ('urg_count', 0.005455290152592193), ('Protocol Type', 0.0045977146366371294)]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Feature Importance - Decision Tree \n",
    "################################################################################\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "n = 100\n",
    "\n",
    "feature_importances = {}\n",
    "for i in range(n):\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    sorted_features = sorted(zip(model.feature_importances_, model.feature_names_in_), reverse=True)\n",
    "    for importance, name in sorted_features:\n",
    "        if name in feature_importances:\n",
    "            feature_importances[name].append(importance)\n",
    "        else:\n",
    "            feature_importances[name] = [importance]\n",
    "\n",
    "average_feature_importances = {}\n",
    "for name, importances in feature_importances.items():\n",
    "    average_feature_importances[name] = sum(importances) / len(importances)\n",
    "\n",
    "top_features = sorted(average_feature_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "with open(f\"results/feature-importance-{sample_size}-dt.txt\", \"a\") as f:\n",
    "    f.write(\"\\n\".join([str(feature) for feature in top_features[:5]]))\n",
    "\n",
    "print(top_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack       1.00      0.99      0.99      1000\n",
      "      normal       0.99      1.00      0.99      1000\n",
      "\n",
      "    accuracy                           0.99      2000\n",
      "   macro avg       0.99      0.99      0.99      2000\n",
      "weighted avg       0.99      0.99      0.99      2000\n",
      "\n",
      "[[ 986   14]\n",
      " [   0 1000]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Predict from Random Forest model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-rf-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rst_count', 0.17119195823251981), ('urg_count', 0.14423365507210814), ('Variance', 0.09524572706704675), ('Tot size', 0.07837678793548522), ('Magnitue', 0.0723118397394615)]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Feature Importance - Random Forest\n",
    "################################################################################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n = 100\n",
    "\n",
    "feature_importances = {}\n",
    "for i in range(n):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    sorted_features = sorted(zip(model.feature_importances_, model.feature_names_in_), reverse=True)\n",
    "    for importance, name in sorted_features:\n",
    "        if name in feature_importances:\n",
    "            feature_importances[name].append(importance)\n",
    "        else:\n",
    "            feature_importances[name] = [importance]\n",
    "\n",
    "average_feature_importances = {}\n",
    "for name, importances in feature_importances.items():\n",
    "    average_feature_importances[name] = sum(importances) / len(importances)\n",
    "\n",
    "top_features = sorted(average_feature_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "with open(f\"results/feature-importance-{sample_size}-rf.txt\", \"a\") as f:\n",
    "    f.write(\"\\n\".join([str(feature) for feature in top_features[:5]]))\n",
    "\n",
    "print(top_features[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Predict from Logistic Regression model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-lr-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Predict from SVM model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-svm-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
