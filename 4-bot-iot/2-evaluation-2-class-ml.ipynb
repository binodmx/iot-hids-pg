{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  8000\n",
      "Test set size:  2000\n",
      "Index(['pkSeqID', 'proto', 'saddr', 'sport', 'daddr', 'dport', 'seq', 'stddev',\n",
      "       'N_IN_Conn_P_SrcIP', 'min', 'state_number', 'mean', 'N_IN_Conn_P_DstIP',\n",
      "       'drate', 'srate', 'max'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Load dataset and split it into training and test set\n",
    "################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sample_size = 10000\n",
    "\n",
    "# Load dateset\n",
    "df_train = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2_train.csv')\n",
    "df_test = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2_test.csv')\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = df_train.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    df_train[column] = label_encoder.fit_transform(df_train[column])\n",
    "    df_test[column] = label_encoder.fit_transform(df_test[column])\n",
    "\n",
    "# Split dataset according to attack type and drop columns\n",
    "normal_df_train = df_train[df_train['attack'] == 0]\n",
    "normal_df_test = df_test[df_test['attack'] == 0]\n",
    "attack_df_train = df_train[df_train['attack'] == 1]\n",
    "attack_df_test = df_test[df_test['attack'] == 1]\n",
    "\n",
    "X_train = pd.concat([normal_df_train, attack_df_train]).drop(columns=['attack', 'category', 'subcategory'])\n",
    "y_train = pd.concat([normal_df_train, attack_df_train])['attack']\n",
    "X_test = pd.concat([normal_df_test, attack_df_test]).drop(columns=['attack', 'category', 'subcategory'])\n",
    "y_test = pd.concat([normal_df_test, attack_df_test])['attack']\n",
    "\n",
    "print(\"Training set size: \", X_train.shape[0])\n",
    "print(\"Test set size: \", X_test.shape[0])\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1000\n",
      "           1       1.00      1.00      1.00      1000\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "[[1000    0]\n",
      " [   0 1000]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Predict from Decision Tree model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-dt-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('N_IN_Conn_P_DstIP', 0.965423066191008), ('pkSeqID', 0.028098293768545925), ('sport', 0.0015548736097067794), ('N_IN_Conn_P_SrcIP', 0.0015548736097067794), ('saddr', 0.0009717960060667373)]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Feature Importance - Decision Tree \n",
    "################################################################################\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "n = 100\n",
    "\n",
    "feature_importances = {}\n",
    "for i in range(n):\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    sorted_features = sorted(zip(model.feature_importances_, model.feature_names_in_), reverse=True)\n",
    "    for importance, name in sorted_features:\n",
    "        if name in feature_importances:\n",
    "            feature_importances[name].append(importance)\n",
    "        else:\n",
    "            feature_importances[name] = [importance]\n",
    "\n",
    "average_feature_importances = {}\n",
    "for name, importances in feature_importances.items():\n",
    "    average_feature_importances[name] = sum(importances) / len(importances)\n",
    "\n",
    "top_features = sorted(average_feature_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "with open(f\"results/feature-importance-{sample_size}-dt.txt\", \"a\") as f:\n",
    "    f.write(\"\\n\".join([str(feature) for feature in top_features[:5]]))\n",
    "\n",
    "print(top_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1000\n",
      "           1       1.00      1.00      1.00      1000\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "[[1000    0]\n",
      " [   0 1000]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Predict from Random Forest model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-rf-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('N_IN_Conn_P_DstIP', 0.2964988829057991), ('pkSeqID', 0.23649864133167045), ('dport', 0.1476154749764546), ('seq', 0.10405859474973063), ('N_IN_Conn_P_SrcIP', 0.0809909503247252)]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Feature Importance - Random Forest\n",
    "################################################################################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n = 100\n",
    "\n",
    "feature_importances = {}\n",
    "for i in range(n):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    sorted_features = sorted(zip(model.feature_importances_, model.feature_names_in_), reverse=True)\n",
    "    for importance, name in sorted_features:\n",
    "        if name in feature_importances:\n",
    "            feature_importances[name].append(importance)\n",
    "        else:\n",
    "            feature_importances[name] = [importance]\n",
    "\n",
    "average_feature_importances = {}\n",
    "for name, importances in feature_importances.items():\n",
    "    average_feature_importances[name] = sum(importances) / len(importances)\n",
    "\n",
    "top_features = sorted(average_feature_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "with open(f\"results/feature-importance-{sample_size}-rf.txt\", \"a\") as f:\n",
    "    f.write(\"\\n\".join([str(feature) for feature in top_features[:5]]))\n",
    "\n",
    "print(top_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1000\n",
      "           1       1.00      0.99      1.00      1000\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "[[1000    0]\n",
      " [   5  995]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S4025371\\OneDrive - RMIT University\\Repositories\\iot-llm\\.conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Predict from Logistic Regression model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-lr-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      1000\n",
      "           1       1.00      0.94      0.97      1000\n",
      "\n",
      "    accuracy                           0.97      2000\n",
      "   macro avg       0.97      0.97      0.97      2000\n",
      "weighted avg       0.97      0.97      0.97      2000\n",
      "\n",
      "[[1000    0]\n",
      " [  65  935]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Predict from SVM model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-svm-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
