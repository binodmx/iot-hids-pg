{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+--------+\n",
      "| Atack type   |   Total |   Train |   Test |\n",
      "+==============+=========+=========+========+\n",
      "| Normal       |      50 |      40 |     10 |\n",
      "+--------------+---------+---------+--------+\n",
      "| Attack       |      50 |      40 |     10 |\n",
      "+--------------+---------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Load dataset and split it into training and test set\n",
    "################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "sample_size = 100\n",
    "\n",
    "# Load dateset\n",
    "df_train = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2_train.csv')\n",
    "df_test = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2_test.csv')\n",
    "\n",
    "# Split dataset according to attack type and drop columns\n",
    "normal_df_train = df_train[df_train['attack'] == 0].drop(columns=['attack', 'category', 'subcategory'])\n",
    "normal_df_test = df_test[df_test['attack'] == 0].drop(columns=['attack', 'category', 'subcategory'])\n",
    "attack_df_train = df_train[df_train['attack'] == 1].drop(columns=['attack', 'category', 'subcategory'])\n",
    "attack_df_test = df_test[df_test['attack'] == 1].drop(columns=['attack', 'category', 'subcategory'])\n",
    "\n",
    "# Print dataset sizes in a table\n",
    "data = [\n",
    "    [\"Normal\", normal_df_train.shape[0] + normal_df_test.shape[0], normal_df_train.shape[0], normal_df_test.shape[0]],\n",
    "    [\"Attack\", attack_df_train.shape[0] + attack_df_test.shape[0], attack_df_train.shape[0], attack_df_test.shape[0]]\n",
    "]\n",
    "print(tabulate(data, headers=[\"Atack type\", \"Total\", \"Train\", \"Test\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Based on the provided normal and attack network data entries, we can observe that the top 3 features that can be used to distinguish between normal and attack entries are:\\n\\n1. `stddev`\\n2. `N_IN_Conn_P_SrcIP`\\n3. `mean`\\n\\nHere are the deterministic rules derived from these features:\\n\\n1. `stddev`: Normal entries have a `stddev` of 0.0, while attack entries have a `stddev` greater than 0.0.\\n2. `N_IN_Conn_P_SrcIP`: Normal entries have `N_IN_Conn_P_SrcIP` values significantly lower than 100, while attack entries have `N_IN_Conn_P_SrcIP` values equal to 100.\\n3. `mean`: Normal entries have a `mean` value close to 0.0, while attack entries have a `mean` value significantly higher than 0.0.\\n\\nUsing these observations, we can create the following JSON format rules:\\n\\n```json\\n{\\n  \"stddev\": \"if stddev == 0.0 then normal else attack\",\\n  \"N_IN_Conn_P_SrcIP\": \"if N_IN_Conn_P_SrcIP < 100 then normal else attack\",\\n  \"mean\": \"if mean < 1.0 then normal else attack\"\\n}\\n```\\n\\nThese rules can be used to classify an entry as normal or attack by evaluating the top 3 features.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 297, 'prompt_tokens': 993, 'total_tokens': 1290}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None} id='run-5df7ee38-2f32-4a87-98e9-6cdcf57b18c2-0' usage_metadata={'input_tokens': 993, 'output_tokens': 297, 'total_tokens': 1290}\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Generate a Rule\n",
    "################################################################################\n",
    "\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "Given normal and attack network data entries, output simple deterministic rule to verify an entry is\n",
    "normal or attack by evaluating top 3 features. Organize your answer into JSON format \n",
    "{{'header1': 'rule', 'header2': 'rule', 'header3': 'rule'}}.\n",
    "\n",
    "Headers:\n",
    "```{headers}```\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"headers\", \"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"bot-iot\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 5})\n",
    "normal_documents = retriever.invoke(str(normal_df_test.iloc[0].to_list()), filter={\"source\": \"bot-iot\", \"label\": \"normal\"})\n",
    "attack_documents = retriever.invoke(str(attack_df_test.iloc[0].to_list()), filter={\"source\": \"bot-iot\", \"label\": \"attack\"})\n",
    "completion = chain.invoke({\n",
    "    \"headers\": normal_df_train.columns.to_list(),\n",
    "    \"normal_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in normal_documents]),\n",
    "    \"attack_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in attack_documents])\n",
    "    })\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting normal entries...: 100%|████████████████████████████████| 10/10 [00:00<00:00, 905.02it/s]\n",
      "Predicting attack entries...: 100%|████████████████████████████████| 10/10 [00:00<00:00, 813.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack       0.91      1.00      0.95        10\n",
      "      normal       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.95        20\n",
      "   macro avg       0.95      0.95      0.95        20\n",
      "weighted avg       0.95      0.95      0.95        20\n",
      "\n",
      "[[10  0]\n",
      " [ 1  9]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Evaluate generated rule\n",
    "################################################################################\n",
    "\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_set_size = 100\n",
    "datasets = {\"normal\": normal_df_test, \"attack\": attack_df_test}\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for attack_type, dataset in datasets.items():\n",
    "    test_set_size = dataset.shape[0]\n",
    "    for i in tqdm(range(test_set_size), ncols=100, desc=f\"Predicting {attack_type} entries...\"):\n",
    "        predicted_attack_type1 = \"attack\" if dataset.iloc[i]['stddev'] > 0 else \"normal\"\n",
    "        predicted_attack_type2 = \"attack\" if dataset.iloc[i]['N_IN_Conn_P_SrcIP'] == 100 else \"normal\"\n",
    "        predicted_attack_type3 = \"attack\" if dataset.iloc[i]['mean'] > 1 else \"normal\"\n",
    "        y_true.append(attack_type)\n",
    "        y_pred.append(mode([predicted_attack_type1, predicted_attack_type2, predicted_attack_type3]))\n",
    "\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-llm-{sample_size}-2-{train_set_size}.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Based on the provided data entries, we can see that normal entries have higher values for features such as flow duration, header length, rate, duration, and total size compared to attack entries. Additionally, normal entries have more occurrences of protocols like HTTP, HTTPS, DNS, SSH, TCP, UDP, DHCP, ARP, ICMP, and IPv.\\n\\nOn the other hand, attack entries have lower values for the mentioned features and do not have as many occurrences of the mentioned protocols. Attack entries also tend to have higher values for features like magnitude, radius, covariance, variance, and weight compared to normal entries.\\n\\nIn summary, normal entries exhibit higher network activity and a wider range of protocols, while attack entries show lower network activity and fewer protocol occurrences.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 149, 'prompt_tokens': 2787, 'total_tokens': 2936}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-93f0cf1f-2f0a-4937-95d9-da6e426ae510-0' usage_metadata={'input_tokens': 2787, 'output_tokens': 149, 'total_tokens': 2936}\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Get a Summary\n",
    "################################################################################\n",
    "\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tiktoken     # https://github.com/openai/tiktoken\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "Given normal and attack network data entries, output human understandable small summary on \n",
    "how attack and normal entries can be simply separated.\n",
    "\n",
    "Headers:\n",
    "```{headers}```\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"headers\", \"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"cic-iot\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 5})\n",
    "normal_documents = retriever.invoke(str(normal_df_test.iloc[0].to_list()), filter={\"source\": \"cic-iot\", \"label\": \"normal\"})\n",
    "attack_documents = retriever.invoke(str(attack_df_test.iloc[0].to_list()), filter={\"source\": \"cic-iot\", \"label\": \"attack\"})\n",
    "completion = chain.invoke({\n",
    "    \"headers\": normal_df_train.columns.to_list(),\n",
    "    \"normal_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in normal_documents]),\n",
    "    \"attack_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in attack_documents])\n",
    "    })\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# \n",
    "################################################################################\n",
    "\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "template = \"\"\"\n",
    "Your task is to identify whether the query is attack or normal. Then \n",
    "generate a policy to filter the given query based on the values. \n",
    "You will be given headers of the entries and similar entries along \n",
    "with the input query to make a decision.\n",
    "\n",
    "Headers:\n",
    "```{headers}```\n",
    "\n",
    "Similar Entries:\n",
    "```{similar_entries}```\n",
    "\n",
    "Input Query: \n",
    "```{query}```\n",
    "\n",
    "Policy:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"headers\", \"similar_entries\", \"query\"])\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"cic-iot\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 5})\n",
    "query_document = str(normal_df_test.iloc[5].to_list())\n",
    "similar_documents = retriever.invoke(query_document, filter={\"source\": \"cic-iot\"})\n",
    "chain.invoke({\n",
    "    \"headers\": normal_df_train.columns.to_list(),\n",
    "    \"similar_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in similar_documents]),\n",
    "    \"query\": query_document\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 2780\n"
     ]
    }
   ],
   "source": [
    "# print(completion.text)\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "num_tokens = len(encoding.encode(str(completion.text)))\n",
    "print(\"Num tokens:\", num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('llm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae27029e1f82e56f496a30980cc01ee9e9874fa5eb01d56792517e894009f269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
