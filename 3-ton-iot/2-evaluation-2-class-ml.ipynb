{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  8000\n",
      "Test set size:  2000\n",
      "Index(['src_ip', 'src_port', 'dst_ip', 'dst_port', 'proto', 'service',\n",
      "       'duration', 'src_bytes', 'dst_bytes', 'conn_state', 'missed_bytes',\n",
      "       'src_pkts', 'src_ip_bytes', 'dst_pkts', 'dst_ip_bytes', 'dns_query',\n",
      "       'dns_qclass', 'dns_qtype', 'dns_rcode', 'dns_AA', 'dns_RD', 'dns_RA',\n",
      "       'dns_rejected', 'ssl_version', 'ssl_cipher', 'ssl_resumed',\n",
      "       'ssl_established', 'ssl_subject', 'ssl_issuer', 'http_trans_depth',\n",
      "       'http_method', 'http_uri', 'http_version', 'http_request_body_len',\n",
      "       'http_response_body_len', 'http_status_code', 'http_user_agent',\n",
      "       'http_orig_mime_types', 'http_resp_mime_types', 'weird_name',\n",
      "       'weird_addl', 'weird_notice'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Load dataset and split it into training and test set\n",
    "################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sample_size = 10000\n",
    "\n",
    "# Load dateset\n",
    "df = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2.csv')\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    df[column] = label_encoder.fit_transform(df[column])\n",
    "\n",
    "# Split dataset according to attack type\n",
    "normal_df = df[df['label'] == 0]\n",
    "attack_df = df[df['label'] == 1]\n",
    "\n",
    "# Split dataset into training and test set\n",
    "normal_df_train = normal_df.sample(frac=0.8, random_state=42)\n",
    "normal_df_test = normal_df.drop(normal_df_train.index)\n",
    "attack_df_train = attack_df.sample(frac=0.8, random_state=42)\n",
    "attack_df_test = attack_df.drop(attack_df_train.index)\n",
    "\n",
    "X_train = pd.concat([normal_df_train, attack_df_train]).drop(columns=['label', 'type'])\n",
    "y_train = pd.concat([normal_df_train, attack_df_train])['label']\n",
    "X_test = pd.concat([normal_df_test, attack_df_test]).drop(columns=['label', 'type'])\n",
    "y_test = pd.concat([normal_df_test, attack_df_test])['label']\n",
    "\n",
    "print(\"Training set size: \", X_train.shape[0])\n",
    "print(\"Test set size: \", X_test.shape[0])\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1000\n",
      "           1       1.00      1.00      1.00      1000\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "[[1000    0]\n",
      " [   0 1000]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Predict from Decision Tree model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-dt-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('src_ip', 0.7861333948849055), ('conn_state', 0.20771105071134244), ('proto', 0.001957047685834498), ('dst_port', 0.0015084602496427027), ('weird_name', 0.0005304474352630123)]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Feature Importance - Decision Tree \n",
    "################################################################################\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "n = 100\n",
    "\n",
    "feature_importances = {}\n",
    "for i in range(n):\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    sorted_features = sorted(zip(model.feature_importances_, model.feature_names_in_), reverse=True)\n",
    "    for importance, name in sorted_features:\n",
    "        if name in feature_importances:\n",
    "            feature_importances[name].append(importance)\n",
    "        else:\n",
    "            feature_importances[name] = [importance]\n",
    "\n",
    "average_feature_importances = {}\n",
    "for name, importances in feature_importances.items():\n",
    "    average_feature_importances[name] = sum(importances) / len(importances)\n",
    "\n",
    "top_features = sorted(average_feature_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "with open(f\"results/feature-importance-{sample_size}-dt.txt\", \"a\") as f:\n",
    "    f.write(\"\\n\".join([str(feature) for feature in top_features[:5]]))\n",
    "\n",
    "print(top_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1000\n",
      "           1       1.00      1.00      1.00      1000\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "[[1000    0]\n",
      " [   0 1000]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Predict from Random Forest model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-rf-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('src_ip', 0.22001233878930151), ('proto', 0.11653590972813657), ('src_ip_bytes', 0.06987228149571628), ('src_pkts', 0.0683003668948703), ('dst_port', 0.05346540903639485)]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Feature Importance - Random Forest\n",
    "################################################################################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n = 100\n",
    "\n",
    "feature_importances = {}\n",
    "for i in range(n):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    sorted_features = sorted(zip(model.feature_importances_, model.feature_names_in_), reverse=True)\n",
    "    for importance, name in sorted_features:\n",
    "        if name in feature_importances:\n",
    "            feature_importances[name].append(importance)\n",
    "        else:\n",
    "            feature_importances[name] = [importance]\n",
    "\n",
    "average_feature_importances = {}\n",
    "for name, importances in feature_importances.items():\n",
    "    average_feature_importances[name] = sum(importances) / len(importances)\n",
    "\n",
    "top_features = sorted(average_feature_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "with open(f\"results/feature-importance-{sample_size}-rf.txt\", \"a\") as f:\n",
    "    f.write(\"\\n\".join([str(feature) for feature in top_features[:5]]))\n",
    "\n",
    "print(top_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.85      1000\n",
      "           1       0.83      0.88      0.86      1000\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n",
      "[[822 178]\n",
      " [118 882]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S4025371\\OneDrive - RMIT University\\Repositories\\iot-llm\\.conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Predict from Logistic Regression model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-lr-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00      1000\n",
      "           1       0.50      1.00      0.67      1000\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.75      0.50      0.33      2000\n",
      "weighted avg       0.75      0.50      0.33      2000\n",
      "\n",
      "[[   1  999]\n",
      " [   0 1000]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Predict from SVM model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-svm-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
