{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+--------+\n",
      "| Atack type   |   Total |   Train |   Test |\n",
      "+==============+=========+=========+========+\n",
      "| Normal       |   50000 |   40000 |  10000 |\n",
      "+--------------+---------+---------+--------+\n",
      "| Attack       |   50000 |   40000 |  10000 |\n",
      "+--------------+---------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Load dataset and split it into training and test set\n",
    "################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "dataset_name = \"wustl-iiot\"\n",
    "sample_size = 100000\n",
    "\n",
    "# Load dateset\n",
    "df = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2.csv')\n",
    "\n",
    "# Split dataset according to attack type\n",
    "normal_df = df[df['Target'] == 0]\n",
    "attack_df = df[df['Target'] == 1]\n",
    "\n",
    "# Drop columns\n",
    "normal_df = normal_df.drop(columns=['Target', 'Traffic'])\n",
    "attack_df = attack_df.drop(columns=['Target', 'Traffic'])\n",
    "\n",
    "# Split dataset into training and test set\n",
    "normal_df_train = normal_df.sample(frac=0.8, random_state=42)\n",
    "normal_df_test = normal_df.drop(normal_df_train.index)\n",
    "attack_df_train = attack_df.sample(frac=0.8, random_state=42)\n",
    "attack_df_test = attack_df.drop(attack_df_train.index)\n",
    "\n",
    "# Print dataset sizes in a table\n",
    "data = [\n",
    "    [\"Normal\", normal_df.shape[0], normal_df_train.shape[0], normal_df_test.shape[0]],\n",
    "    [\"Attack\", attack_df.shape[0], attack_df_train.shape[0], attack_df_test.shape[0]]\n",
    "]\n",
    "print(tabulate(data, headers=[\"Atack type\", \"Total\", \"Train\", \"Test\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Generate Feature Importance\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Output top 10 important features that can be used to filter an entry as either normal or attack.\n",
    "Output only in the Python list structure.\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\n",
    "Example output:\n",
    "['feature1', 'feature2', 'feature3', ..., 'feature10']\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "model_name = \"gpt-4o\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "# llm = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "# model_name = \"claude-3-opus-20240229\"\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=dataset_name,\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "\n",
    "normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "normal_entries = {}\n",
    "for i, feature_name in enumerate(normal_df_train.columns.to_list()):\n",
    "    normal_entries[feature_name] = [json.loads(doc)[i] for doc in normal_documents]\n",
    "\n",
    "attack_entries = {}\n",
    "for i, feature_name in enumerate(attack_df_train.columns.to_list()):\n",
    "    attack_entries[feature_name] = [json.loads(doc)[i] for doc in attack_documents]\n",
    "\n",
    "completions = []\n",
    "for i in range(10):\n",
    "    completion = chain.invoke({\n",
    "        \"normal_entries\": json.dumps(normal_entries),\n",
    "        \"attack_entries\": json.dumps(attack_entries)\n",
    "    })\n",
    "    completions.append(completion.content)\n",
    "    print(completion.content)\n",
    "    time.sleep(10)\n",
    "\n",
    "with open(f\"results/feature-importance-{sample_size}-llm-{model_name}.txt\", \"a\") as f:\n",
    "    f.write(\"\\n\".join(completions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S4025371\\OneDrive - RMIT University\\Repositories\\iot-llm\\.conda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"SrcAddr\": \"If SrcAddr is 192.168.0.20, then Normal; otherwise, Attack\",\n",
      "    \"Dport\": \"If Dport is 502, then Normal; otherwise, Attack\",\n",
      "    \"SrcPkts\": \"If SrcPkts is 10, then Normal; otherwise, Attack\",\n",
      "    \"DstPkts\": \"If DstPkts is 8, then Normal; otherwise, Attack\",\n",
      "    \"TotBytes\": \"If TotBytes is 1152, then Normal; otherwise, Attack\"\n",
      "}\n",
      "```\n",
      "Prompt tokens: 5289\n",
      "Completion tokens: 118\n",
      "Total tokens: 5407\n",
      "Percentage of tokens used: 0.0422421875\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Generate Rules with transposed data\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import uuid\n",
    "import tiktoken     # https://github.com/openai/tiktoken\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Generate 5 simple and deterministic rules for top 5 important features to filter an entry as either normal or attack. \n",
    "Output only in the JSON format with the structure: \n",
    "{{'feature1': 'rule', 'feature2': 'rule', ..., 'feature5': 'rule'}}.\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "model_name = \"gpt-4o\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "# llm = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "# model_name = \"claude-3-opus-20240229\"\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=dataset_name,\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "\n",
    "normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "normal_entries = {}\n",
    "for i, feature_name in enumerate(normal_df_train.columns.to_list()):\n",
    "    normal_entries[feature_name] = [json.loads(doc.replace(\"'\", \"\\\"\"))[i] for doc in normal_documents]\n",
    "\n",
    "attack_entries = {}\n",
    "for i, feature_name in enumerate(attack_df_train.columns.to_list()):\n",
    "    attack_entries[feature_name] = [json.loads(doc.replace(\"'\", \"\\\"\"))[i] for doc in attack_documents]\n",
    "\n",
    "# prompt_text = prompt.invoke({\n",
    "#     \"normal_entries\": json.dumps(normal_entries),\n",
    "#     \"attack_entries\": json.dumps(attack_entries)\n",
    "# }).text\n",
    "\n",
    "# print(prompt_text)\n",
    "\n",
    "completion = chain.invoke({\n",
    "    \"normal_entries\": json.dumps(normal_entries),\n",
    "    \"attack_entries\": json.dumps(attack_entries)\n",
    "})\n",
    "\n",
    "print(completion.content)\n",
    "\n",
    "id = str(uuid.uuid4())\n",
    "with open(f\"results/llm/generated-rules-{sample_size}-llm-{model_name}.txt\", \"a\") as f:\n",
    "    f.write(f\"{id}\\n\")\n",
    "    f.write(f\"{completion.content}\\n\")\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "num_tokens_prompt = len(encoding.encode(prompt.invoke({\"normal_entries\": json.dumps(normal_entries),\"attack_entries\": json.dumps(attack_entries)}).text))\n",
    "num_tokens_completion = len(encoding.encode(str(completion.content)))\n",
    "\n",
    "print(f\"Prompt tokens: {num_tokens_prompt}\")\n",
    "print(f\"Completion tokens: {num_tokens_completion}\")\n",
    "print(f\"Total tokens: {num_tokens_prompt + num_tokens_completion}\")\n",
    "print(f\"Percentage of tokens used: {(num_tokens_prompt + num_tokens_completion) / 128000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting normal entries...: 100%|███████████████████████████| 1000/1000 [00:00<00:00, 3017.50it/s]\n",
      "Predicting attack entries...: 100%|███████████████████████████| 1000/1000 [00:00<00:00, 2987.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack     0.8658    1.0000    0.9281      1000\n",
      "      normal     1.0000    0.8450    0.9160      1000\n",
      "\n",
      "    accuracy                         0.9225      2000\n",
      "   macro avg     0.9329    0.9225    0.9220      2000\n",
      "weighted avg     0.9329    0.9225    0.9220      2000\n",
      "\n",
      "[[1000    0]\n",
      " [ 155  845]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Evaluate generated rules\n",
    "################################################################################\n",
    "\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = {\"normal\": normal_df_test, \"attack\": attack_df_test}\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for attack_type, dataset in datasets.items():\n",
    "    test_set_size = dataset.shape[0]\n",
    "    for i in tqdm(range(test_set_size), ncols=100, desc=f\"Predicting {attack_type} entries...\"):\n",
    "        predicted_attack_types = []\n",
    "        predicted_attack_types.append(\"normal\" if dataset.iloc[i]['SrcAddr'] == \"192.168.0.20\" else \"attack\")\n",
    "        predicted_attack_types.append(\"normal\" if dataset.iloc[i]['Dport'] == 502 else \"attack\")\n",
    "        predicted_attack_types.append(\"normal\" if dataset.iloc[i]['SrcPkts'] == 10 else \"attack\")\n",
    "        predicted_attack_types.append(\"normal\" if dataset.iloc[i]['DstPkts'] == 8 else \"attack\")\n",
    "        predicted_attack_types.append(\"normal\" if dataset.iloc[i]['TotBytes'] == 1152 else \"attack\")\n",
    "        y_true.append(attack_type)\n",
    "        y_pred.append(mode(predicted_attack_types))\n",
    "\n",
    "c_report = classification_report(y_true, y_pred, digits=4)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/llm/result-llm-{sample_size}-2.txt\", \"a\") as f:\n",
    "    f.write(f\"{id}\\n\")\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\\n\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Prompt Template\n",
    "################################################################################\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "system_message = (\"system\",\n",
    "\"\"\"\n",
    "You are a good data analyst.\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Your task is to generate {k} simple and deterministic rules for top {k} important features to filter attack entries.\n",
    "Supported operators are '==', '!=', '>', '<', '>=', '<='.\n",
    "Generate exactly {k} rules to filter attack entries and make a tool call for each rule.\n",
    "\"\"\"\n",
    ")\n",
    "human_message = (\"user\",\n",
    "\"\"\"\n",
    "Analyze the following network data and generate rules for the top 5 important features to filter attack entries.\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    human_message,\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "# Invoke prompt\n",
    "# prompt.invoke({\"k\": 5, \"normal_entries\": normal_entries, \"attack_entries\": attack_entries, \"msgs\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Tool\n",
    "################################################################################\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import operator\n",
    "from typing import Annotated\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "show_progress = True\n",
    "operations = {'<': operator.lt, '>': operator.gt, '==': operator.eq, '<=': operator.le, '>=': operator.ge, '!=': operator.ne}\n",
    "\n",
    "@tool\n",
    "def evaluate_rule(\n",
    "    feature_name: Annotated[str, \"Feature name\"],\n",
    "    value: Annotated[str, \"Value\"], \n",
    "    op: Annotated[str, \"Operator\"]\n",
    ") -> bool:\n",
    "    \"\"\"Evaluate the rule and return the macro f1-score.\"\"\"\n",
    "    try:\n",
    "        value = float(value)\n",
    "    except ValueError:\n",
    "        value\n",
    "    datasets = {\"normal\": normal_df_train, \"attack\": attack_df_train}\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    if op in operations:\n",
    "        for attack_type, dataset in datasets.items():\n",
    "            test_set_size = dataset.shape[0]\n",
    "            for i in tqdm(range(test_set_size), ncols=100, desc=f\"Predicting {attack_type} entries...\", disable=not show_progress):\n",
    "                y_true.append(attack_type)\n",
    "                y_pred.append(\"attack\" if operations[op](dataset.iloc[i][feature_name], value) else \"normal\")\n",
    "        c_report = classification_report(y_true, y_pred, digits=4, output_dict=True)\n",
    "        return c_report['macro avg']['f1-score']\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported operator: {op}\")\n",
    "\n",
    "# Invoke tool\n",
    "# print(evaluate_rule.invoke({\"feature_name\": \"flow_duration\", \"value\": \"1\", \"op\": \"<\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# LLM\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "llm = ChatOpenAI(model=model_name, temperature=0.1)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "# llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.0)\n",
    "# model_name = \"claude-3-opus-20240229\"\n",
    "# llm = ChatAnthropic(model=model_name, temperature=0.0)\n",
    "\n",
    "llm_with_tool = llm.bind_tools([evaluate_rule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Vector Store\n",
    "################################################################################\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "train_set_size = sample_size\n",
    "n_results = 10\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=dataset_name,\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "\n",
    "normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=n_results)['documents'][0]\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=n_results)['documents'][0]\n",
    "\n",
    "normal_entries_dict = {}\n",
    "for i, feature_name in enumerate(normal_df_train.columns.to_list()):\n",
    "    normal_entries_dict[feature_name] = [json.loads(doc.replace(\"'\", \"\\\"\"))[i] for doc in normal_documents]\n",
    "\n",
    "attack_entries_dict = {}\n",
    "for i, feature_name in enumerate(attack_df_train.columns.to_list()):\n",
    "    attack_entries_dict[feature_name] = [json.loads(doc.replace(\"'\", \"\\\"\"))[i] for doc in attack_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 1 Current mean f1-score: 0.8655846577728055 Token usage: {'completion_tokens': 453, 'prompt_tokens': 5367, 'total_tokens': 5820}\n",
      "Round: 2 Current mean f1-score: 0.8363417979066222 Token usage: {'completion_tokens': 481, 'prompt_tokens': 5996, 'total_tokens': 6477}\n",
      "Round: 3 Current mean f1-score: 0.8627639312673082 Token usage: {'completion_tokens': 398, 'prompt_tokens': 6653, 'total_tokens': 7051}\n",
      "Round: 4 Current mean f1-score: 0.940612908306159 Token usage: {'completion_tokens': 391, 'prompt_tokens': 7207, 'total_tokens': 7598}\n",
      "Round: 5 Current mean f1-score: 0.9365228303168088 Token usage: {'completion_tokens': 394, 'prompt_tokens': 7773, 'total_tokens': 8167}\n",
      "[0.8655846577728055, 0.8363417979066222, 0.8627639312673082, 0.940612908306159, 0.9365228303168088]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Chain\n",
    "################################################################################\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chain = prompt | llm_with_tool\n",
    "\n",
    "n_repetitions = 5\n",
    "context_window = 128000\n",
    "show_progress = False\n",
    "\n",
    "def get_initial_state():\n",
    "  n = 0\n",
    "  k = 5\n",
    "  mean_f1s = 0\n",
    "  max_f1s = 0\n",
    "  n_max = 0\n",
    "  token_usage = {}\n",
    "  normal_entries = json.dumps(normal_entries_dict)\n",
    "  attack_entries = json.dumps(attack_entries_dict)\n",
    "  msgs = []\n",
    "  return locals()\n",
    "\n",
    "state = get_initial_state()\n",
    "train_f1_scores = []\n",
    "while state[\"n\"] < n_repetitions:\n",
    "    ai_msg = chain.invoke(state)\n",
    "    tool_msgs = []\n",
    "    for tool_call in ai_msg.tool_calls:\n",
    "        tool_msg = evaluate_rule.invoke(tool_call)\n",
    "        tool_msgs.append(tool_msg)\n",
    "    state[\"mean_f1s\"] = sum(float(msg.content) for msg in tool_msgs) / len(tool_msgs)\n",
    "    human_msg = HumanMessage(f\"The current mean f1-score for the generated rules is {state['mean_f1s']}. \"\n",
    "                             \"If this mean f1-score is greater than the previous rounds, keep the better performing \"\n",
    "                             \"rules and revise or replace only the underperforming ones (those with a score less than mean). \"\n",
    "                             \"Otherwise, revise or replace any rules that have a score less than mean. \"\n",
    "                             f\"Based on the feedback, generate exactly {state['k']} rules to filter attack entries and \"\n",
    "                             \"make a tool call for each rule, ensuring that a tool call is made for every entry every time.\")\n",
    "    state[\"n\"] += 1\n",
    "    state[\"msgs\"].extend([ai_msg, *tool_msgs, human_msg])\n",
    "    train_f1_scores.append(state[\"mean_f1s\"])\n",
    "    state[\"max_f1s\"] = state[\"mean_f1s\"] if state[\"mean_f1s\"] > state[\"max_f1s\"] else state[\"max_f1s\"]\n",
    "    state[\"n_max\"] = state[\"n\"] if state[\"mean_f1s\"] > state[\"max_f1s\"] else state[\"n_max\"]\n",
    "    state[\"token_usage\"] = {key: ai_msg.response_metadata[\"token_usage\"][key] for key in [\"completion_tokens\", \"prompt_tokens\", \"total_tokens\"]} \n",
    "    print(\"Round:\", state[\"n\"], \"Current mean f1-score:\", state[\"mean_f1s\"], \"Token usage:\", state[\"token_usage\"])\n",
    "\n",
    "print(train_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack if SrcAddr != 192.168.0.20 else normal\n",
      "attack if Dport == 80 else normal\n",
      "attack if SrcPkts == 2 else normal\n",
      "attack if DstPkts == 0 else normal\n",
      "attack if TotBytes == 124 else normal\n",
      "0.8632729612271635\n",
      "attack if SrcAddr != 192.168.0.20 else normal\n",
      "attack if SrcRate == 1000000 else normal\n",
      "attack if DstRate == 0 else normal\n",
      "attack if DstPkts == 0 else normal\n",
      "attack if Rate == 1000000 else normal\n",
      "0.9747964906633599\n",
      "attack if SrcAddr != 192.168.0.20 else normal\n",
      "attack if SrcLoad == 496000000 else normal\n",
      "attack if DstRate == 0 else normal\n",
      "0.9758460116330558\n",
      "attack if SrcAddr != 192.168.0.20 else normal\n",
      "attack if SrcBytes == 124 else normal\n",
      "attack if DstRate == 0 else normal\n",
      "attack if DstPkts == 0 else normal\n",
      "attack if DstLoad == 0 else normal\n",
      "0.9713480968689643\n",
      "attack if SrcAddr != 192.168.0.20 else normal\n",
      "attack if TotPkts == 2 else normal\n",
      "attack if DstRate == 0 else normal\n",
      "attack if DstPkts == 0 else normal\n",
      "attack if DstLoad == 0 else normal\n",
      "0.9713480968689643\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Evaluate generated rules\n",
    "################################################################################\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import operator\n",
    "from statistics import mode\n",
    "\n",
    "operations = {'<': operator.lt, '>': operator.gt, '==': operator.eq, '<=': operator.le, '>=': operator.ge, '!=': operator.ne}\n",
    "\n",
    "def evaluate_rules(tool_calls):\n",
    "    datasets = {\"normal\": normal_df_test, \"attack\": attack_df_test}\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for attack_type, dataset in datasets.items():\n",
    "        test_set_size = dataset.shape[0]\n",
    "        for i in tqdm(range(test_set_size), ncols=100, desc=f\"Predicting {attack_type} entries...\", disable=not show_progress):\n",
    "            predicted_attack_types = []\n",
    "            for tool_call in tool_calls:\n",
    "                args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "                op = args[\"op\"]\n",
    "                feature_name = args[\"feature_name\"]\n",
    "                value = args[\"value\"]\n",
    "                try:\n",
    "                    value = float(value)\n",
    "                except ValueError:\n",
    "                    value\n",
    "                predicted_attack_types.append(\"attack\" if operations[op](dataset.iloc[i][feature_name], value) else \"normal\")\n",
    "            y_true.append(attack_type)\n",
    "            y_pred.append(mode(predicted_attack_types))\n",
    "    c_report = classification_report(y_true, y_pred, digits=4, output_dict=True)\n",
    "    c_matrix = confusion_matrix(y_true, y_pred)\n",
    "    # print(c_report)\n",
    "    # print(c_matrix)\n",
    "    return c_report\n",
    "\n",
    "# tool_calls = state[\"msgs\"][-7].additional_kwargs[\"tool_calls\"]\n",
    "# for tool_call in tool_calls:\n",
    "#     rule = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "#     print(\"attack if\", rule[\"feature_name\"], rule[\"op\"], rule[\"value\"], \"else normal\")\n",
    "\n",
    "# evaluate_rules(tool_calls)\n",
    "\n",
    "# test_f1_scores = []\n",
    "# for i in range(20, 0, -1):\n",
    "#     index = -7 * i\n",
    "#     tool_calls = state[\"msgs\"][index].additional_kwargs[\"tool_calls\"]\n",
    "#     for tool_call in tool_calls:\n",
    "#         rule = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "#     test_f1_scores.append(evaluate_rules(tool_calls)['macro avg']['f1-score'])\n",
    "\n",
    "# print(test_f1_scores)\n",
    "\n",
    "for i in range(len(state[\"msgs\"])):\n",
    "    if state[\"msgs\"][i].type != \"ai\":\n",
    "        continue\n",
    "    tool_calls = state[\"msgs\"][i].additional_kwargs[\"tool_calls\"]\n",
    "    for tool_call in tool_calls:\n",
    "        rule = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "        print(\"attack if\", rule[\"feature_name\"], rule[\"op\"], rule[\"value\"], \"else normal\")\n",
    "    c_report = evaluate_rules(tool_calls)\n",
    "    print(c_report[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT time taken: 0.001401893651485443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000     10000\n",
      "           1     1.0000    1.0000    1.0000     10000\n",
      "\n",
      "    accuracy                         1.0000     20000\n",
      "   macro avg     1.0000    1.0000    1.0000     20000\n",
      "weighted avg     1.0000    1.0000    1.0000     20000\n",
      "\n",
      "[[10000     0]\n",
      " [    0 10000]]\n",
      "\n",
      "\n",
      "RF time taken: 0.016973010945320128\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9999    0.9999    0.9999     10000\n",
      "           1     0.9999    0.9999    0.9999     10000\n",
      "\n",
      "    accuracy                         0.9999     20000\n",
      "   macro avg     0.9999    0.9999    0.9999     20000\n",
      "weighted avg     0.9999    0.9999    0.9999     20000\n",
      "\n",
      "[[9999    1]\n",
      " [   1 9999]]\n",
      "\n",
      "\n",
      "LLM time taken: 0.000884722650051117\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack     0.9796    0.9632    0.9713     10000\n",
      "      normal     0.9638    0.9799    0.9718     10000\n",
      "\n",
      "    accuracy                         0.9716     20000\n",
      "   macro avg     0.9717    0.9715    0.9715     20000\n",
      "weighted avg     0.9717    0.9716    0.9715     20000\n",
      "\n",
      "[[9632  368]\n",
      " [ 201 9799]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Evaluate generated rules for efficiency\n",
    "################################################################################\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tabulate import tabulate\n",
    "from statistics import mode\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sample_size = 100000\n",
    "\n",
    "# Load dateset\n",
    "df = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2.csv')\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    df[column] = label_encoder.fit_transform(df[column])\n",
    "\n",
    "# Split dataset according to attack type\n",
    "normal_df = df[df['Target'] == 0]\n",
    "attack_df = df[df['Target'] == 1]\n",
    "\n",
    "# Split dataset into training and test set\n",
    "normal_df_train = normal_df.sample(frac=0.8, random_state=42)\n",
    "normal_df_test = normal_df.drop(normal_df_train.index)\n",
    "attack_df_train = attack_df.sample(frac=0.8, random_state=42)\n",
    "attack_df_test = attack_df.drop(attack_df_train.index)\n",
    "\n",
    "X_train = pd.concat([normal_df_train, attack_df_train]).drop(columns=['Target', 'Traffic'])\n",
    "y_train = pd.concat([normal_df_train, attack_df_train])['Target']\n",
    "X_test = pd.concat([normal_df_test, attack_df_test]).drop(columns=['Target', 'Traffic'])\n",
    "y_test = pd.concat([normal_df_test, attack_df_test])['Target']\n",
    "\n",
    "# Create instances of ML models\n",
    "model_dt = DecisionTreeClassifier()\n",
    "model_rf = RandomForestClassifier()\n",
    "\n",
    "# Fit the models to the training data\n",
    "model_dt.fit(X_train, y_train)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "\n",
    "elapsed_times_dt = []\n",
    "elapsed_times_rf = []\n",
    "elapsed_times_llm = []\n",
    "y_pred_dt = []\n",
    "y_pred_rf = []\n",
    "y_pred_llm = []\n",
    "for i in range(len(X_test)):\n",
    "    # Predict using DT\n",
    "    start = time.time()\n",
    "    y_pred_dt.append(model_dt.predict([X_test.iloc[i]]))\n",
    "    end = time.time()\n",
    "    elapsed_times_dt.append(end - start)\n",
    "\n",
    "    # Predict using RF\n",
    "    start = time.time()\n",
    "    y_pred_rf.append(model_rf.predict([X_test.iloc[i]]))\n",
    "    end = time.time()\n",
    "    elapsed_times_rf.append(end - start)\n",
    "    \n",
    "    # Predict using LLM\n",
    "    start = time.time()\n",
    "    row = X_test.iloc[i]\n",
    "    # conditions = [\n",
    "    #     row['SrcAddr'] != \"192.168.0.20\",\n",
    "    #     row['SrcRate'] == 1000000,\n",
    "    #     row['DstRate'] == 0,\n",
    "    #     row['DstPkts'] == 0,\n",
    "    #     row['Rate'] == 1000000\n",
    "    # ]\n",
    "    conditions = [\n",
    "        row['SrcAddr'] != \"192.168.0.20\",\n",
    "        row['TotPkts'] == 2,\n",
    "        row['DstRate'] == 0,\n",
    "        row['DstPkts'] == 0,\n",
    "        row['DstLoad'] == 0\n",
    "    ]\n",
    "    predicted_attack_types = [\"attack\" if condition else \"normal\" for condition in conditions]\n",
    "    y_pred_llm.append(mode(predicted_attack_types))\n",
    "    end = time.time()\n",
    "    elapsed_times_llm.append(end - start)\n",
    "\n",
    "print(f\"DT time taken: {sum(elapsed_times_dt)/len(X_test)}\")\n",
    "print(classification_report(y_true, y_pred_dt, digits=4, output_dict=False))\n",
    "print(confusion_matrix(y_true, y_pred_dt))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"RF time taken: {sum(elapsed_times_rf)/len(X_test)}\")\n",
    "print(classification_report(y_true, y_pred_rf, digits=4, output_dict=False))\n",
    "print(confusion_matrix(y_true, y_pred_rf))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"LLM time taken: {sum(elapsed_times_llm)/len(X_test)}\\n\")\n",
    "print(classification_report([\"attack\" if y else \"normal\" for y in y_true], y_pred_llm, digits=4, output_dict=False))\n",
    "print(confusion_matrix([\"attack\" if y else \"normal\" for y in y_true], y_pred_llm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
