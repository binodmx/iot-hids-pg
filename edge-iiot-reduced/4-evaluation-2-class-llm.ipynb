{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+--------+\n",
      "| Atack type   |   Total |   Train |   Test |\n",
      "+==============+=========+=========+========+\n",
      "| Normal       |    5000 |    4000 |   1000 |\n",
      "+--------------+---------+---------+--------+\n",
      "| Attack       |    5000 |    4000 |   1000 |\n",
      "+--------------+---------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Load dataset and split it into training and test set\n",
    "################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "sample_size = 10000\n",
    "\n",
    "# Load dateset\n",
    "df = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2.csv')\n",
    "\n",
    "# Split dataset according to attack type\n",
    "normal_df = df[df['Attack_label'] == 0]\n",
    "attack_df = df[df['Attack_label'] == 1]\n",
    "\n",
    "# Drop columns\n",
    "normal_df = normal_df.drop(columns=['Attack_label', 'Attack_type'])\n",
    "attack_df = attack_df.drop(columns=['Attack_label', 'Attack_type'])\n",
    "\n",
    "# Split dataset into training and test set\n",
    "normal_df_train = normal_df.sample(frac=0.8, random_state=42)\n",
    "normal_df_test = normal_df.drop(normal_df_train.index)\n",
    "attack_df_train = attack_df.sample(frac=0.8, random_state=42)\n",
    "attack_df_test = attack_df.drop(attack_df_train.index)\n",
    "\n",
    "# Print dataset sizes in a table\n",
    "data = [\n",
    "    [\"Normal\", normal_df.shape[0], normal_df_train.shape[0], normal_df_test.shape[0]],\n",
    "    [\"Attack\", attack_df.shape[0], attack_df_train.shape[0], attack_df_test.shape[0]]\n",
    "]\n",
    "print(tabulate(data, headers=[\"Atack type\", \"Total\", \"Train\", \"Test\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Feature Importance\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "import time\n",
    "import numpy as np\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Output top 5 important features that can be used to filter an entry as either normal or attack.\n",
    "Output only in the Python list structure.\n",
    "\n",
    "Feature Names:\n",
    "```{feature_names}```\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\n",
    "Example output:\n",
    "['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"feature_names\", \"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "model_name = \"gpt-4o\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"edge-iiotset\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "\n",
    "normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "completions = []\n",
    "for i in range(10):\n",
    "    completion = chain.invoke({\n",
    "        \"feature_names\": normal_df_train.columns.to_list(),\n",
    "        \"normal_entries\": \",\\n\".join([f\"{doc} --> normal\" for doc in normal_documents]),\n",
    "        \"attack_entries\": \",\\n\".join([f\"{doc} --> attack\" for doc in attack_documents])\n",
    "        })\n",
    "    completions.append(completion.content)\n",
    "    print(completion.content)\n",
    "    time.sleep(10)\n",
    "\n",
    "with open(f\"results/feature-importance-{sample_size}-llm-{model_name}.txt\", \"a\") as f:\n",
    "    f.write(\"\\n\".join(completions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S4025371\\OneDrive - RMIT University\\Repositories\\iot-llm\\.conda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"tcp.dstport\": \"if tcp.dstport == 80.0 then attack else normal\",\n",
      "  \"tcp.flags\": \"if tcp.flags == 18.0 then attack else normal\",\n",
      "  \"tcp.options\": \"if tcp.options != '0' then attack else normal\",\n",
      "  \"tcp.ack\": \"if tcp.ack == 1.0 then attack else normal\",\n",
      "  \"tcp.len\": \"if tcp.len > 100.0 then attack else normal\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Generate Rules\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Generate 5 simple and deterministic rules for top 5 important features to filter an entry as either normal or attack. \n",
    "Output only in the JSON format with the structure: \n",
    "{{'feature1': 'rule', 'feature2': 'rule', ..., 'feature5': 'rule'}}.\n",
    "\n",
    "Feature Names:\n",
    "```{feature_names}```\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"feature_names\", \"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "model_name = \"gpt-4o\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "# llm = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "# model_name = \"claude-3-opus-20240229\"\n",
    "# chain = prompt\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"edge-iiotset\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db3-{train_set_size}-2\")\n",
    "\n",
    "normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=10)['documents']\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=10)['documents']\n",
    "\n",
    "completion = chain.invoke({\n",
    "    \"feature_names\": normal_df_train.columns.to_list(),\n",
    "    \"normal_entries\": \",\\n\".join([f\"{doc} --> normal\" for doc in normal_documents]),\n",
    "    \"attack_entries\": \",\\n\".join([f\"{doc} --> attack\" for doc in attack_documents])\n",
    "    })\n",
    "\n",
    "# print(completion)\n",
    "print(completion.content)\n",
    "\n",
    "# with open(f\"results/generated-rules-{sample_size}-llm-{model_name}.txt\", \"a\") as f:\n",
    "#     f.write(completion.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting normal entries...: 100%|███████████████████████████| 1000/1000 [00:00<00:00, 2557.52it/s]\n",
      "Predicting attack entries...: 100%|███████████████████████████| 1000/1000 [00:00<00:00, 2604.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack       0.85      0.25      0.39      1000\n",
      "      normal       0.56      0.95      0.71      1000\n",
      "\n",
      "    accuracy                           0.60      2000\n",
      "   macro avg       0.70      0.60      0.55      2000\n",
      "weighted avg       0.70      0.60      0.55      2000\n",
      "\n",
      "[[252 748]\n",
      " [ 45 955]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate generated rule\n",
    "################################################################################\n",
    "\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = {\"normal\": normal_df_test, \"attack\": attack_df_test}\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for attack_type, dataset in datasets.items():\n",
    "    test_set_size = dataset.shape[0]\n",
    "    for i in tqdm(range(test_set_size), ncols=100, desc=f\"Predicting {attack_type} entries...\"):\n",
    "        predicted_attack_types = []\n",
    "        # print(type(dataset.iloc[i]['mqtt.msgtype']))\n",
    "        # predicted_attack_types.append(\"normal\" if float(dataset.iloc[i]['dns.qry.name.len']) < 50 else \"attack\")\n",
    "        # predicted_attack_types.append(\"normal\" if float(dataset.iloc[i]['icmp.seq_le']) < 1000 else \"attack\")\n",
    "        # predicted_attack_types.append(\"normal\" if dataset.iloc[i]['dns.retransmission'] == '0' else \"attack\")\n",
    "        # predicted_attack_types.append(\"normal\" if dataset.iloc[i]['mqtt.protoname'] != 'MQTT' else \"attack\")\n",
    "        # predicted_attack_types.append(\"normal\" if dataset.iloc[i]['mqtt.msg'] == '0' else \"attack\")\n",
    "        # predicted_attack_types.append(\"normal\" if dataset.iloc[i]['mqtt.msgtype'] in [1,2,3] else \"attack\")\n",
    "        # predicted_attack_types.append(\"normal\" if dataset.iloc[i]['mqtt.conack.flags'] == '0' else \"attack\")\n",
    "        # predicted_attack_types.append(\"normal\" if dataset.iloc[i]['mqtt.hdrflags'] == '0' else \"attack\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.ack'] == 1 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.connection.fin'] == 1.0 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.connection.syn'] == 1.0 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.connection.synack'] == 1.0 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.connection.rst'] == 1.0 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.dstport'] in [80] else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.flags'] in [18.0] else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.options'] != '0' else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.seq'] > 1000000000.0 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.srcport'] == 80.0 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.len'] > 100 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.ack_raw'] < 1000000000.0 else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.payload'] != '0' else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['ip.src_host'] == \"192.168.0.170\" else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['ip.dst_host'] == \"192.168.0.170\" else \"normal\")\n",
    "        # predicted_attack_types.append(\"normal\" if dataset.iloc[i]['http.request.method'] in [\"GET\", \"POST\"] else \"attack\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['http.request.uri.query'] == \"0\" else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['http.referer'] == \"0\" else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['http.request.full_uri'] == \"0\" else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if dataset.iloc[i]['udp.port'] == 80 else \"normal\")\n",
    "        y_true.append(attack_type)\n",
    "        y_pred.append(mode(predicted_attack_types))\n",
    "\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-llm-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S4025371\\OneDrive - RMIT University\\Repositories\\iot-llm\\.conda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"ip.src_host\": \"if ip.src_host == '192.168.0.128' and ip.dst_host == '192.168.0.170' then 'attack' else 'normal'\",\n",
      "  \"tcp.ack\": \"if tcp.ack in [1.0, 262.0, 122.0, 446.0, 320.0] then 'attack' else 'normal'\",\n",
      "  \"tcp.dstport\": \"if tcp.dstport in [46616.0, 43030.0, 59964.0, 44672.0, 59716.0, 45208.0, 42468.0, 51916.0, 34892.0, 41524.0] then 'attack' else 'normal'\",\n",
      "  \"tcp.flags\": \"if tcp.flags in [17.0, 18.0] then 'attack' else 'normal'\",\n",
      "  \"tcp.srcport\": \"if tcp.srcport == '80.0' then 'attack' else 'normal'\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Generate Rules with transposed data\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze and identify patterns between normal and attack entries by comparing corresponding fields.\n",
    "Generate 5 signature-based rules for top 5 important features to filter an entry as either normal or attack. \n",
    "Output only in the JSON format with the structure: \n",
    "{{'feature1': 'rule', 'feature2': 'rule', ..., 'feature5': 'rule'}}.\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "model_name = \"gpt-4o\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "# llm = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "# model_name = \"claude-3-opus-20240229\"\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"edge-iiotset\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "\n",
    "normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=10)['documents'][0]\n",
    "\n",
    "normal_entries = {}\n",
    "for i, feature_name in enumerate(normal_df_train.columns.to_list()):\n",
    "    normal_entries[feature_name] = [json.loads(doc.replace(\"'\", \"\\\"\"))[i] for doc in normal_documents]\n",
    "\n",
    "attack_entries = {}\n",
    "for i, feature_name in enumerate(attack_df_train.columns.to_list()):\n",
    "    attack_entries[feature_name] = [json.loads(doc.replace(\"'\", \"\\\"\"))[i] for doc in attack_documents]\n",
    "\n",
    "completion = chain.invoke({\n",
    "    \"feature_names\": normal_df_train.columns.to_list(),\n",
    "    \"normal_entries\": json.dumps(normal_entries),\n",
    "    \"attack_entries\": json.dumps(attack_entries)\n",
    "    })\n",
    "\n",
    "print(completion.content)\n",
    "\n",
    "with open(f\"results/generated-rules-{sample_size}-llm-{model_name}.txt\", \"a\") as f:\n",
    "    f.write(completion.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting normal entries...: 100%|██████████████████████████| 1000/1000 [00:00<00:00, 13677.87it/s]\n",
      "Predicting attack entries...: 100%|██████████████████████████| 1000/1000 [00:00<00:00, 15588.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack       1.00      1.00      1.00      1000\n",
      "      normal       1.00      1.00      1.00      1000\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "[[ 999    1]\n",
      " [   0 1000]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate generated rule\n",
    "################################################################################\n",
    "\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = {\"normal\": normal_df_test, \"attack\": attack_df_test}\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for attack_type, dataset in datasets.items():\n",
    "    test_set_size = dataset.shape[0]\n",
    "    for i in tqdm(range(test_set_size), ncols=100, desc=f\"Predicting {attack_type} entries...\"):\n",
    "        predicted_attack_types = []\n",
    "        # predicted_attack_types.append(\"attack\" if (dataset.iloc[i]['ip.src_host'] == \"192.168.0.128\") and (dataset.iloc[i]['ip.dst_host'] == \"192.168.0.170\") else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if str(dataset.iloc[i]['tcp.srcport']) == \"80.0\" else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if str(dataset.iloc[i]['tcp.flags']) == \"18.0\" else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if str(dataset.iloc[i]['tcp.ack']) == \"1.0\" else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if str(dataset.iloc[i]['tcp.dstport']) != \"1883.0\" else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if str(dataset.iloc[i]['http.request.method']) not in [\"6.0\", \"59.0\"] else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if str(dataset.iloc[i]['tcp.ack_raw']) not in [\"1883.0\", \"60500.0\", \"58731.0\"] else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if str(dataset.iloc[i]['tcp.checksum']) == \"18.0\" else \"normal\")\n",
    "        # predicted_attack_types.append(\"attack\" if str(dataset.iloc[i]['tcp.options']) != \"0\" else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if (dataset.iloc[i]['dns.qry.name.len']) == '0.0' else \"normal\")\n",
    "        y_true.append(attack_type)\n",
    "        y_pred.append(mode(predicted_attack_types))\n",
    "\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-llm-{sample_size}-2.txt\", \"a\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frame.time': [' 2021 17:19:23.748023000 ',\n",
       "  ' 2021 15:41:43.312269000 ',\n",
       "  ' 2021 20:26:22.889420000 ',\n",
       "  ' 2021 14:31:22.580423000 ',\n",
       "  ' 2021 19:30:32.025270000 ',\n",
       "  ' 2021 16:21:11.840867000 ',\n",
       "  ' 2021 14:24:49.475107000 ',\n",
       "  ' 2021 16:04:05.260009000 ',\n",
       "  ' 2021 21:00:11.914284000 ',\n",
       "  ' 2021 14:29:47.500017000 '],\n",
       " 'ip.src_host': ['192.168.0.101',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.101',\n",
       "  '192.168.0.101',\n",
       "  '192.168.0.101',\n",
       "  '192.168.0.101',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.101',\n",
       "  '192.168.0.101',\n",
       "  '192.168.0.101'],\n",
       " 'ip.dst_host': ['192.168.0.128',\n",
       "  '192.168.0.101',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.101',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.128'],\n",
       " 'arp.src.proto_ipv4': ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       " 'icmp.checksum': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'icmp.seq_le': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'http.file_data': ['0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0'],\n",
       " 'http.content_length': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'http.request.uri.query': ['0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0'],\n",
       " 'http.request.method': ['0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0'],\n",
       " 'http.referer': ['0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0'],\n",
       " 'http.request.full_uri': ['0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0'],\n",
       " 'http.request.version': ['0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0'],\n",
       " 'tcp.ack': [6.0, 59.0, 6.0, 6.0, 6.0, 6.0, 59.0, 6.0, 6.0, 6.0],\n",
       " 'tcp.ack_raw': [1733134321.0,\n",
       "  4245847939.0,\n",
       "  4096341123.0,\n",
       "  258802568.0,\n",
       "  1972850038.0,\n",
       "  2865572786.0,\n",
       "  226221498.0,\n",
       "  1918327287.0,\n",
       "  3674683869.0,\n",
       "  1167151939.0],\n",
       " 'tcp.checksum': [43148.0,\n",
       "  54919.0,\n",
       "  14086.0,\n",
       "  15121.0,\n",
       "  48972.0,\n",
       "  49868.0,\n",
       "  52950.0,\n",
       "  27614.0,\n",
       "  14757.0,\n",
       "  24829.0],\n",
       " 'tcp.connection.rst': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'tcp.connection.syn': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'tcp.connection.synack': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'tcp.dstport': [1883.0,\n",
       "  60500.0,\n",
       "  1883.0,\n",
       "  1883.0,\n",
       "  1883.0,\n",
       "  1883.0,\n",
       "  58731.0,\n",
       "  1883.0,\n",
       "  1883.0,\n",
       "  1883.0],\n",
       " 'tcp.flags': [16.0, 17.0, 16.0, 16.0, 16.0, 16.0, 17.0, 16.0, 16.0, 16.0],\n",
       " 'tcp.flags.ack': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " 'tcp.len': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'tcp.options': ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       " 'tcp.payload': ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       " 'tcp.seq': [59.0, 5.0, 59.0, 59.0, 59.0, 59.0, 5.0, 59.0, 59.0, 59.0],\n",
       " 'tcp.srcport': ['50223.0',\n",
       "  '1883.0',\n",
       "  '50964.0',\n",
       "  '59149.0',\n",
       "  '57253.0',\n",
       "  '52470.0',\n",
       "  '1883.0',\n",
       "  '65243.0',\n",
       "  '53304.0',\n",
       "  '59055.0'],\n",
       " 'udp.port': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'udp.stream': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'dns.qry.name.len': ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       " 'dns.retransmission': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.conack.flags': ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       " 'mqtt.conflag.cleansess': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.conflags': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.hdrflags': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.len': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.msg': ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       " 'mqtt.msgtype': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.proto_len': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.protoname': ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       " 'mqtt.topic': ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       " 'mqtt.topic_len': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.ver': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frame.time': [' 2021 18:53:58.406720000 ',\n",
       "  ' 2021 21:41:18.683777000 ',\n",
       "  ' 2021 18:42:50.392712000 ',\n",
       "  ' 2021 21:42:40.085690000 ',\n",
       "  ' 2021 18:27:36.407423000 ',\n",
       "  ' 2021 21:43:03.475400000 ',\n",
       "  ' 2021 21:40:51.402154000 ',\n",
       "  ' 2021 20:40:19.090082000 ',\n",
       "  ' 2021 19:47:30.509174000 ',\n",
       "  ' 2021 21:40:11.073360000 '],\n",
       " 'ip.src_host': ['192.168.0.128',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.128',\n",
       "  '192.168.0.128'],\n",
       " 'ip.dst_host': ['192.168.0.170',\n",
       "  '192.168.0.170',\n",
       "  '192.168.0.170',\n",
       "  '192.168.0.170',\n",
       "  '192.168.0.170',\n",
       "  '192.168.0.170',\n",
       "  '192.168.0.170',\n",
       "  '192.168.0.170',\n",
       "  '192.168.0.170',\n",
       "  '192.168.0.170'],\n",
       " 'arp.src.proto_ipv4': ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       " 'icmp.checksum': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'icmp.seq_le': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'http.file_data': ['0',\n",
       "  '0.0',\n",
       "  '0',\n",
       "  '0.0',\n",
       "  '0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0.0'],\n",
       " 'http.content_length': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'http.request.uri.query': ['0.0',\n",
       "  '0',\n",
       "  '0.0',\n",
       "  '0',\n",
       "  '0.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0'],\n",
       " 'http.request.method': ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       " 'http.referer': ['0.0',\n",
       "  '0.0',\n",
       "  '0',\n",
       "  '0.0',\n",
       "  '0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0'],\n",
       " 'http.request.full_uri': ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       " 'http.request.version': ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       " 'tcp.ack': [262.0, 1.0, 122.0, 446.0, 1.0, 320.0, 1.0, 262.0, 1.0, 1.0],\n",
       " 'tcp.ack_raw': [22107710.0,\n",
       "  2812004323.0,\n",
       "  880463836.0,\n",
       "  3861443883.0,\n",
       "  3258075469.0,\n",
       "  2544831743.0,\n",
       "  2974994290.0,\n",
       "  1845244820.0,\n",
       "  1475353760.0,\n",
       "  610488797.0],\n",
       " 'tcp.checksum': [37764.0,\n",
       "  51306.0,\n",
       "  56385.0,\n",
       "  2721.0,\n",
       "  8135.0,\n",
       "  608.0,\n",
       "  43277.0,\n",
       "  34249.0,\n",
       "  30535.0,\n",
       "  48986.0],\n",
       " 'tcp.connection.rst': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'tcp.connection.syn': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'tcp.connection.synack': [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0],\n",
       " 'tcp.dstport': [46616.0,\n",
       "  43030.0,\n",
       "  59964.0,\n",
       "  44672.0,\n",
       "  59716.0,\n",
       "  45208.0,\n",
       "  42468.0,\n",
       "  51916.0,\n",
       "  34892.0,\n",
       "  41524.0],\n",
       " 'tcp.flags': [16.0, 18.0, 17.0, 17.0, 18.0, 17.0, 18.0, 17.0, 18.0, 18.0],\n",
       " 'tcp.flags.ack': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " 'tcp.len': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'tcp.options': ['0101080a7b3de40ce7c8f3d6',\n",
       "  '020405b40402080a9eb178c59b23e5ec01030307',\n",
       "  '0101080a9e0e13829a8080c5',\n",
       "  '0101080a9eb2b6bf9b2523c4',\n",
       "  '020405b40402080a9e0021419a728e2e01030307',\n",
       "  '0101080a9eb3121d9b257f45',\n",
       "  '020405b40402080a9eb10e349b237b7c01030307',\n",
       "  '0101080a7b9f427de82a5966',\n",
       "  '020405b40402080a7b6ee938e7f9fa1601030307',\n",
       "  '020405b40402080a9eb070ab9b22de1201030307'],\n",
       " 'tcp.payload': ['0', '0', '0', '0', '0', '0', '0', '0', '0', '0'],\n",
       " 'tcp.seq': [304.0, 0.0, 485.0, 478.0, 0.0, 478.0, 0.0, 303.0, 0.0, 0.0],\n",
       " 'tcp.srcport': ['80.0',\n",
       "  '80.0',\n",
       "  '80.0',\n",
       "  '80.0',\n",
       "  '80.0',\n",
       "  '80.0',\n",
       "  '80.0',\n",
       "  '80.0',\n",
       "  '80.0',\n",
       "  '80.0'],\n",
       " 'udp.port': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'udp.stream': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'dns.qry.name.len': ['0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0'],\n",
       " 'dns.retransmission': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.conack.flags': ['0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0'],\n",
       " 'mqtt.conflag.cleansess': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.conflags': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.hdrflags': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.len': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.msg': ['0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0'],\n",
       " 'mqtt.msgtype': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.proto_len': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.protoname': ['0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0'],\n",
       " 'mqtt.topic': ['0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0'],\n",
       " 'mqtt.topic_len': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'mqtt.ver': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "['tcp.dstport', 'tcp.flags', 'tcp.seq', 'tcp.ack', 'tcp.len']\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Generate Feature Importance\n",
    "################################################################################\n",
    "\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Generate top 5 features that can be used to filter an entry as either normal or attack.\n",
    "Output only in the Python list structure.\n",
    "\n",
    "Feature Names:\n",
    "```{feature_names}```\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\n",
    "Example output:\n",
    "['feature', 'feature2', 'feature3', 'feature4', 'feature5']\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"feature_names\", \"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"edge-iiotset\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 10, \"fetch_k\": 10})\n",
    "normal_documents = retriever.invoke(str(normal_df_test.iloc[0].to_list()), filter={\"source\": \"edge-iiotset\", \"label\": \"normal\"})\n",
    "attack_documents = retriever.invoke(str(attack_df_test.iloc[0].to_list()), filter={\"source\": \"edge-iiotset\", \"label\": \"attack\"})\n",
    "completion = chain.invoke({\n",
    "    \"feature_names\": normal_df_train.columns.to_list(),\n",
    "    \"normal_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in normal_documents]),\n",
    "    \"attack_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in attack_documents])\n",
    "    })\n",
    "print(completion.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Based on the provided data entries, we can see that normal entries have higher values for features such as flow duration, header length, rate, duration, and total size compared to attack entries. Additionally, normal entries have more occurrences of protocols like HTTP, HTTPS, DNS, SSH, TCP, UDP, DHCP, ARP, ICMP, and IPv.\\n\\nOn the other hand, attack entries have lower values for the mentioned features and do not have as many occurrences of the mentioned protocols. Attack entries also tend to have higher values for features like magnitude, radius, covariance, variance, and weight compared to normal entries.\\n\\nIn summary, normal entries exhibit higher network activity and a wider range of protocols, while attack entries show lower network activity and fewer protocol occurrences.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 149, 'prompt_tokens': 2787, 'total_tokens': 2936}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-93f0cf1f-2f0a-4937-95d9-da6e426ae510-0' usage_metadata={'input_tokens': 2787, 'output_tokens': 149, 'total_tokens': 2936}\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Get a Summary\n",
    "################################################################################\n",
    "\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tiktoken     # https://github.com/openai/tiktoken\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "Given normal and attack network data entries, output human understandable small summary on \n",
    "how attack and normal entries can be simply separated.\n",
    "\n",
    "Feature Names:\n",
    "```{feature_names}```\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"feature_names\", \"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"edge-iiotset\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 5})\n",
    "normal_documents = retriever.invoke(str(normal_df_test.iloc[0].to_list()), filter={\"source\": \"edge-iiotset\", \"label\": \"normal\"})\n",
    "attack_documents = retriever.invoke(str(attack_df_test.iloc[0].to_list()), filter={\"source\": \"edge-iiotset\", \"label\": \"attack\"})\n",
    "completion = chain.invoke({\n",
    "    \"feature_names\": normal_df_train.columns.to_list(),\n",
    "    \"normal_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in normal_documents]),\n",
    "    \"attack_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in attack_documents])\n",
    "    })\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# \n",
    "################################################################################\n",
    "\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "template = \"\"\"\n",
    "Your task is to identify whether the query is attack or normal. Then \n",
    "generate a policy to filter the given query based on the values. \n",
    "You will be given feature names of the entries and similar entries along \n",
    "with the input query to make a decision.\n",
    "\n",
    "Feature Names:\n",
    "```{feature_names}```\n",
    "\n",
    "Similar Entries:\n",
    "```{similar_entries}```\n",
    "\n",
    "Input Query: \n",
    "```{query}```\n",
    "\n",
    "Policy:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"feature_names\", \"similar_entries\", \"query\"])\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"edge-iiotset\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 5})\n",
    "query_document = str(normal_df_test.iloc[5].to_list())\n",
    "similar_documents = retriever.invoke(query_document, filter={\"source\": \"edge-iiotset\"})\n",
    "chain.invoke({\n",
    "    \"feature_names\": normal_df_train.columns.to_list(),\n",
    "    \"similar_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in similar_documents]),\n",
    "    \"query\": query_document\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 2780\n"
     ]
    }
   ],
   "source": [
    "# print(completion.text)\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "num_tokens = len(encoding.encode(str(completion.text)))\n",
    "print(\"Num tokens:\", num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S4025371\\OneDrive - RMIT University\\Repositories\\iot-llm\\.conda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attack', 'attack', 'attack', 'attack', 'attack', 'attack', 'attack', 'attack', 'attack', 'attack']\n",
      "['normal', 'normal', 'normal', 'normal', 'normal']\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Generate Rules with transposed data\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding feature names.\n",
    "Carefully analyze and identify patterns between normal and attack entries by comparing corresponding fields.\n",
    "Generate 5 signature-based rules for top 5 important features to filter an entry as either normal or attack. \n",
    "Output only in the JSON format with the structure: \n",
    "{{'feature1': 'rule', 'feature2': 'rule', ..., 'feature5': 'rule'}}.\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "model_name = \"gpt-4o\"\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.0)\n",
    "# model_name = \"gemini-1.5-pro\"\n",
    "# llm = ChatAnthropic(model='claude-3-opus-20240229')\n",
    "# model_name = \"claude-3-opus-20240229\"\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"edge-iiotset\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db3-{train_set_size}-2\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 5})\n",
    "\n",
    "\n",
    "\n",
    "# normal_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'normal'})['embeddings']\n",
    "# normal_mean_vector = np.mean(normal_vectors, axis=0).tolist()\n",
    "# normal_documents = vector_store._collection.query(query_embeddings=[normal_mean_vector], n_results=10)['metadatas'][0]\n",
    "\n",
    "attack_vectors = vector_store._collection.get(include=['embeddings'], where={'label': 'attack'})['embeddings']\n",
    "attack_mean_vector = np.mean(attack_vectors, axis=0).tolist()\n",
    "attack_documents = vector_store._collection.query(query_embeddings=[attack_mean_vector], n_results=10)['metadatas'][0]\n",
    "\n",
    "query_document = str(attack_mean_vector)\n",
    "similar_documents = retriever.invoke(query_document, filter={'label': 'attack'})\n",
    "\n",
    "# print([x['label'] for x in normal_documents])\n",
    "print([x['label'] for x in attack_documents])\n",
    "print([doc.metadata['label'] for doc in similar_documents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('llm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae27029e1f82e56f496a30980cc01ee9e9874fa5eb01d56792517e894009f269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
