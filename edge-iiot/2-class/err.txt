Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards:  12%|█▎        | 1/8 [04:01<28:11, 241.66s/it]Downloading shards:  25%|██▌       | 2/8 [07:16<21:23, 213.91s/it]Downloading shards:  38%|███▊      | 3/8 [30:41<1:03:10, 758.07s/it]Downloading shards:  50%|█████     | 4/8 [48:00<57:55, 868.84s/it]  Downloading shards:  62%|██████▎   | 5/8 [51:10<31:12, 624.09s/it]Downloading shards:  75%|███████▌  | 6/8 [55:07<16:25, 492.60s/it]Downloading shards:  88%|████████▊ | 7/8 [58:22<06:35, 395.07s/it]Downloading shards: 100%|██████████| 8/8 [58:44<00:00, 276.31s/it]Downloading shards: 100%|██████████| 8/8 [58:44<00:00, 440.53s/it]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:04<00:32,  4.59s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:08<00:24,  4.13s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:11<00:19,  3.82s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:16<00:16,  4.03s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:19<00:11,  3.86s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:23<00:07,  3.72s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:26<00:03,  3.67s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:28<00:00,  3.07s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:28<00:00,  3.57s/it]
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (8192). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Traceback (most recent call last):
  File "/opt/home/s4025371/repositories/iot-llm/edge-iiot/2-class/3-predict-from-llm.py", line 123, in <module>
    y = predict(llm, str(normal_df_test.iloc[i].to_list()))
  File "/opt/home/s4025371/repositories/iot-llm/edge-iiot/2-class/3-predict-from-llm.py", line 99, in predict
    y_pred = chain.invoke({"input": x})
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 2878, in invoke
    input = context.run(step.invoke, input, config)
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 344, in invoke
    self.generate_prompt(
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 701, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 880, in generate
    output = self._generate_helper(
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 738, in _generate_helper
    raise e
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/llms.py", line 725, in _generate_helper
    self._generate(
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_huggingface/llms/huggingface_pipeline.py", line 269, in _generate
    responses = self.pipeline(
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/text_generation.py", line 262, in __call__
    return super().__call__(text_inputs, **kwargs)
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/base.py", line 1238, in __call__
    outputs = list(final_iterator)
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py", line 124, in __next__
    item = next(self.iterator)
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py", line 125, in __next__
    processed = self.infer(item, **self.params)
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/base.py", line 1164, in forward
    model_outputs = self._forward(model_inputs, **forward_params)
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/pipelines/text_generation.py", line 351, in _forward
    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py", line 2024, in generate
    result = self._sample(
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py", line 2982, in _sample
    outputs = self(**model_inputs, return_dict=True)
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/accelerate/hooks.py", line 169, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py", line 1019, in forward
    logits = logits.float()
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.89 GiB. GPU 0 has a total capacity of 44.46 GiB of which 5.55 GiB is free. Including non-PyTorch memory, this process has 38.89 GiB memory in use. Of the allocated memory 19.96 GiB is allocated by PyTorch, and 18.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
