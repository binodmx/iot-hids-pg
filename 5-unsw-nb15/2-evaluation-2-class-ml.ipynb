{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  8000\n",
      "Test set size:  2000\n",
      "Index(['id', 'dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',\n",
      "       'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss',\n",
      "       'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',\n",
      "       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
      "       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
      "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
      "       'ct_srv_dst', 'is_sm_ips_ports'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Load dataset and split it into training and test set\n",
    "################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sample_size = 10000\n",
    "\n",
    "# Load dateset\n",
    "df_train = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2_train.csv')\n",
    "df_test = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2_test.csv')\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = df_train.select_dtypes(include=['object']).columns\n",
    "for column in categorical_columns:\n",
    "    df_train[column] = label_encoder.fit_transform(df_train[column])\n",
    "    df_test[column] = label_encoder.fit_transform(df_test[column])\n",
    "\n",
    "# Split dataset according to attack type and drop columns\n",
    "normal_df_train = df_train[df_train['label'] == 0]\n",
    "normal_df_test = df_test[df_test['label'] == 0]\n",
    "attack_df_train = df_train[df_train['label'] == 1]\n",
    "attack_df_test = df_test[df_test['label'] == 1]\n",
    "\n",
    "X_train = pd.concat([normal_df_train, attack_df_train]).drop(columns=['attack_cat', 'label'])\n",
    "y_train = pd.concat([normal_df_train, attack_df_train])['label']\n",
    "X_test = pd.concat([normal_df_test, attack_df_test]).drop(columns=['attack_cat', 'label'])\n",
    "y_test = pd.concat([normal_df_test, attack_df_test])['label']\n",
    "\n",
    "print(\"Training set size: \", X_train.shape[0])\n",
    "print(\"Test set size: \", X_test.shape[0])\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.71      0.61      1000\n",
      "           1       0.58      0.39      0.47      1000\n",
      "\n",
      "    accuracy                           0.55      2000\n",
      "   macro avg       0.56      0.55      0.54      2000\n",
      "weighted avg       0.56      0.55      0.54      2000\n",
      "\n",
      "[[712 288]\n",
      " [609 391]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Predict from Decision Tree model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-dt-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 0.8129995491608345), ('is_sm_ips_ports', 0.07031723740165942), ('sload', 0.028997315722627114), ('ct_srv_dst', 0.017523395831451433), ('ct_dst_src_ltm', 0.010367409956935983)]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Feature Importance - Decision Tree \n",
    "################################################################################\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "n = 100\n",
    "\n",
    "feature_importances = {}\n",
    "for i in range(n):\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    sorted_features = sorted(zip(model.feature_importances_, model.feature_names_in_), reverse=True)\n",
    "    for importance, name in sorted_features:\n",
    "        if name in feature_importances:\n",
    "            feature_importances[name].append(importance)\n",
    "        else:\n",
    "            feature_importances[name] = [importance]\n",
    "\n",
    "average_feature_importances = {}\n",
    "for name, importances in feature_importances.items():\n",
    "    average_feature_importances[name] = sum(importances) / len(importances)\n",
    "\n",
    "top_features = sorted(average_feature_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "with open(f\"results/feature-importance-{sample_size}-dt.txt\", \"a\") as f:\n",
    "    f.write(\"\\n\".join([str(feature) for feature in top_features[:5]]))\n",
    "\n",
    "print(top_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.75      0.71      1000\n",
      "           1       0.72      0.63      0.67      1000\n",
      "\n",
      "    accuracy                           0.69      2000\n",
      "   macro avg       0.69      0.69      0.69      2000\n",
      "weighted avg       0.69      0.69      0.69      2000\n",
      "\n",
      "[[752 248]\n",
      " [369 631]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Predict from Random Forest model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-rf-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 0.23401143977770444), ('sttl', 0.11226248109954706), ('ct_state_ttl', 0.0845238981149299), ('sload', 0.05215648273790055), ('dload', 0.04459302308972417)]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Feature Importance - Random Forest\n",
    "################################################################################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n = 100\n",
    "\n",
    "feature_importances = {}\n",
    "for i in range(n):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    sorted_features = sorted(zip(model.feature_importances_, model.feature_names_in_), reverse=True)\n",
    "    for importance, name in sorted_features:\n",
    "        if name in feature_importances:\n",
    "            feature_importances[name].append(importance)\n",
    "        else:\n",
    "            feature_importances[name] = [importance]\n",
    "\n",
    "average_feature_importances = {}\n",
    "for name, importances in feature_importances.items():\n",
    "    average_feature_importances[name] = sum(importances) / len(importances)\n",
    "\n",
    "top_features = sorted(average_feature_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "with open(f\"results/feature-importance-{sample_size}-rf.txt\", \"a\") as f:\n",
    "    f.write(\"\\n\".join([str(feature) for feature in top_features[:5]]))\n",
    "\n",
    "print(top_features[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.51      0.59      1000\n",
      "           1       0.61      0.77      0.68      1000\n",
      "\n",
      "    accuracy                           0.64      2000\n",
      "   macro avg       0.65      0.64      0.63      2000\n",
      "weighted avg       0.65      0.64      0.63      2000\n",
      "\n",
      "[[509 491]\n",
      " [231 769]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\S4025371\\OneDrive - RMIT University\\Repositories\\iot-llm\\.conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Predict from Logistic Regression model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-lr-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.69      1000\n",
      "           1       0.70      0.66      0.68      1000\n",
      "\n",
      "    accuracy                           0.69      2000\n",
      "   macro avg       0.69      0.69      0.69      2000\n",
      "weighted avg       0.69      0.69      0.69      2000\n",
      "\n",
      "[[711 289]\n",
      " [340 660]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Predict from SVM model\n",
    "################################################################################\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create an instance of the DecisionTreeClassifier model\n",
    "model = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_true = y_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-svm-{sample_size}-2.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
