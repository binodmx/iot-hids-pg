{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+--------+\n",
      "| Atack type   |   Total |   Train |   Test |\n",
      "+==============+=========+=========+========+\n",
      "| Normal       |      50 |      40 |     10 |\n",
      "+--------------+---------+---------+--------+\n",
      "| Attack       |      50 |      40 |     10 |\n",
      "+--------------+---------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Load dataset and split it into training and test set\n",
    "################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "sample_size = 100\n",
    "\n",
    "# Load dateset\n",
    "df = pd.read_csv(os.getcwd() + f'/data/sample-{sample_size}-2.csv')\n",
    "\n",
    "# Split dataset according to attack type\n",
    "normal_df = df[df['Attack_label'] == 0]\n",
    "attack_df = df[df['Attack_label'] == 1]\n",
    "\n",
    "# Drop columns\n",
    "normal_df = normal_df.drop(columns=['Attack_label', 'Attack_type'])\n",
    "attack_df = attack_df.drop(columns=['Attack_label', 'Attack_type'])\n",
    "\n",
    "# Split dataset into training and test set\n",
    "normal_df_train = normal_df.sample(frac=0.8, random_state=42)\n",
    "normal_df_test = normal_df.drop(normal_df_train.index)\n",
    "attack_df_train = attack_df.sample(frac=0.8, random_state=42)\n",
    "attack_df_test = attack_df.drop(attack_df_train.index)\n",
    "\n",
    "# Print dataset sizes in a table\n",
    "data = [\n",
    "    [\"Normal\", normal_df.shape[0], normal_df_train.shape[0], normal_df_test.shape[0]],\n",
    "    [\"Attack\", attack_df.shape[0], attack_df_train.shape[0], attack_df_test.shape[0]]\n",
    "]\n",
    "print(tabulate(data, headers=[\"Atack type\", \"Total\", \"Train\", \"Test\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```json\\n{\\n  \"ip.src_host\": \"if ip.src_host is \\'192.168.0.170\\' then attack else normal\",\\n  \"tcp.dstport\": \"if tcp.dstport is 80.0 then attack else normal\",\\n  \"tcp.len\": \"if tcp.len > 100.0 then attack else normal\",\\n  \"tcp.seq\": \"if tcp.seq is not \\'0\\' then attack else normal\",\\n  \"tcp.srcport\": \"if tcp.srcport is \\'80.0\\' then attack else normal\"\\n}\\n```' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 113, 'prompt_tokens': 5982, 'total_tokens': 6095}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None} id='run-9063190e-8db0-4b47-a3b8-5b26d52ab3a7-0' usage_metadata={'input_tokens': 5982, 'output_tokens': 113, 'total_tokens': 6095}\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Generate a Rule\n",
    "################################################################################\n",
    "\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are given normal and attack network data entries along with corresponding headers.\n",
    "Think step by step and identify differences between normal and attack entries.\n",
    "Output 5 simple deterministic rules to verify an entry is normal or attack. \n",
    "Only output in JSON format {{'header1': 'rule', 'header2': 'rule', 'header3': 'rule', 'header4': 'rule', 'header5': 'rule'}}.\n",
    "\n",
    "Headers:\n",
    "```{headers}```\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"headers\", \"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"edge-iiotset\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 10, \"fetch_k\": 10})\n",
    "normal_documents = retriever.invoke(str(normal_df_test.iloc[0].to_list()), filter={\"source\": \"edge-iiotset\", \"label\": \"normal\"})\n",
    "attack_documents = retriever.invoke(str(attack_df_test.iloc[0].to_list()), filter={\"source\": \"edge-iiotset\", \"label\": \"attack\"})\n",
    "completion = chain.invoke({\n",
    "    \"headers\": normal_df_train.columns.to_list(),\n",
    "    \"normal_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in normal_documents]),\n",
    "    \"attack_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in attack_documents])\n",
    "    })\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting normal entries...: 100%|███████████████████████████| 1000/1000 [00:00<00:00, 2264.22it/s]\n",
      "Predicting attack entries...: 100%|███████████████████████████| 1000/1000 [00:00<00:00, 2377.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      attack       1.00      0.00      0.00      1000\n",
      "      normal       0.50      1.00      0.67      1000\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.75      0.50      0.34      2000\n",
      "weighted avg       0.75      0.50      0.34      2000\n",
      "\n",
      "[[   2  998]\n",
      " [   0 1000]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Evaluate generated rule\n",
    "################################################################################\n",
    "\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# train_set_size = 100\n",
    "datasets = {\"normal\": normal_df_test, \"attack\": attack_df_test}\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for attack_type, dataset in datasets.items():\n",
    "    test_set_size = dataset.shape[0]\n",
    "    predicted_attack_types = []\n",
    "    for i in tqdm(range(test_set_size), ncols=100, desc=f\"Predicting {attack_type} entries...\"):\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['ip.src_host'] == \"192.168.0.170\" else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.seq'] != 0 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.srcport'] == 80 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.dstport'] == 80 else \"normal\")\n",
    "        predicted_attack_types.append(\"attack\" if dataset.iloc[i]['tcp.len'] > 100 else \"normal\")\n",
    "        y_true.append(attack_type)\n",
    "        y_pred.append(mode(predicted_attack_types))\n",
    "\n",
    "c_report = classification_report(y_true, y_pred)\n",
    "c_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "with open(f\"results/result-llm-{sample_size}-2-{train_set_size}.txt\", \"w\") as f:\n",
    "    f.write(f\"Classication Report\\n{c_report}\\n\\nConfusion Matrix\\n{c_matrix}\")\n",
    "\n",
    "print(c_report)\n",
    "print(c_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "['tcp.dstport', 'tcp.flags', 'tcp.seq', 'tcp.ack', 'tcp.len']\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Generate Feature Importance\n",
    "################################################################################\n",
    "\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "You are provided with network data entries categorized as either normal or attack, along with their corresponding headers.\n",
    "Carefully analyze the differences between normal and attack entries by comparing corresponding fields.\n",
    "Generate top 5 features that can be used to filter an entry as either normal or attack.\n",
    "Output only in the Python list structure.\n",
    "\n",
    "Headers:\n",
    "```{headers}```\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\n",
    "Example output:\n",
    "['feature', 'feature2', 'feature3', 'feature4', 'feature5']\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"headers\", \"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"edge-iiotset\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 10, \"fetch_k\": 10})\n",
    "normal_documents = retriever.invoke(str(normal_df_test.iloc[0].to_list()), filter={\"source\": \"edge-iiotset\", \"label\": \"normal\"})\n",
    "attack_documents = retriever.invoke(str(attack_df_test.iloc[0].to_list()), filter={\"source\": \"edge-iiotset\", \"label\": \"attack\"})\n",
    "completion = chain.invoke({\n",
    "    \"headers\": normal_df_train.columns.to_list(),\n",
    "    \"normal_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in normal_documents]),\n",
    "    \"attack_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in attack_documents])\n",
    "    })\n",
    "print(completion.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/home/s4025371/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Based on the provided data entries, we can see that normal entries have higher values for features such as flow duration, header length, rate, duration, and total size compared to attack entries. Additionally, normal entries have more occurrences of protocols like HTTP, HTTPS, DNS, SSH, TCP, UDP, DHCP, ARP, ICMP, and IPv.\\n\\nOn the other hand, attack entries have lower values for the mentioned features and do not have as many occurrences of the mentioned protocols. Attack entries also tend to have higher values for features like magnitude, radius, covariance, variance, and weight compared to normal entries.\\n\\nIn summary, normal entries exhibit higher network activity and a wider range of protocols, while attack entries show lower network activity and fewer protocol occurrences.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 149, 'prompt_tokens': 2787, 'total_tokens': 2936}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-93f0cf1f-2f0a-4937-95d9-da6e426ae510-0' usage_metadata={'input_tokens': 2787, 'output_tokens': 149, 'total_tokens': 2936}\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Get a Summary\n",
    "################################################################################\n",
    "\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tiktoken     # https://github.com/openai/tiktoken\n",
    "\n",
    "dotenv.load_dotenv(os.getcwd() + '/../.env')\n",
    "\n",
    "template = \"\"\"\n",
    "Given normal and attack network data entries, output human understandable small summary on \n",
    "how attack and normal entries can be simply separated.\n",
    "\n",
    "Headers:\n",
    "```{headers}```\n",
    "\n",
    "Normal Entries:\n",
    "```{normal_entries}```\n",
    "\n",
    "Attack Entries:\n",
    "```{attack_entries}```\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"headers\", \"normal_entries\", \"attack_entries\"])\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"cic-iot\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 5})\n",
    "normal_documents = retriever.invoke(str(normal_df_test.iloc[0].to_list()), filter={\"source\": \"cic-iot\", \"label\": \"normal\"})\n",
    "attack_documents = retriever.invoke(str(attack_df_test.iloc[0].to_list()), filter={\"source\": \"cic-iot\", \"label\": \"attack\"})\n",
    "completion = chain.invoke({\n",
    "    \"headers\": normal_df_train.columns.to_list(),\n",
    "    \"normal_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in normal_documents]),\n",
    "    \"attack_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in attack_documents])\n",
    "    })\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# \n",
    "################################################################################\n",
    "\n",
    "import dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from statistics import mode\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "template = \"\"\"\n",
    "Your task is to identify whether the query is attack or normal. Then \n",
    "generate a policy to filter the given query based on the values. \n",
    "You will be given headers of the entries and similar entries along \n",
    "with the input query to make a decision.\n",
    "\n",
    "Headers:\n",
    "```{headers}```\n",
    "\n",
    "Similar Entries:\n",
    "```{similar_entries}```\n",
    "\n",
    "Input Query: \n",
    "```{query}```\n",
    "\n",
    "Policy:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"headers\", \"similar_entries\", \"query\"])\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")\n",
    "chain = prompt | llm\n",
    "train_set_size = sample_size\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"cic-iot\",\n",
    "    embedding_function=embeddings, \n",
    "    persist_directory=f\"./vector-stores/chroma-db-{train_set_size}-2\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 5})\n",
    "query_document = str(normal_df_test.iloc[5].to_list())\n",
    "similar_documents = retriever.invoke(query_document, filter={\"source\": \"cic-iot\"})\n",
    "chain.invoke({\n",
    "    \"headers\": normal_df_train.columns.to_list(),\n",
    "    \"similar_entries\": \",\\n\".join([f\"{doc.page_content} --> {doc.metadata['label']}\" for doc in similar_documents]),\n",
    "    \"query\": query_document\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 2780\n"
     ]
    }
   ],
   "source": [
    "# print(completion.text)\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "num_tokens = len(encoding.encode(str(completion.text)))\n",
    "print(\"Num tokens:\", num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('llm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae27029e1f82e56f496a30980cc01ee9e9874fa5eb01d56792517e894009f269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
